{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/daltongabrielomondi/zpsn-transformer?scriptVersionId=273153028\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# GPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Data: Full CIFAR-10\ntransform = transforms.Compose([\n    transforms.ToTensor(), \n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n\n# Helper classes for ViT\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n        self.dropout = nn.Dropout(dropout)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: t.reshape(x.shape[0], -1, self.heads, qkv[0].shape[-1] // self.heads).transpose(1, 2), qkv)\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).reshape(x.shape[0], -1, out.shape[-1] * self.heads)\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads=heads, dropout=dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\n# ViT Model\nclass ViT(nn.Module):\n    def __init__(self, image_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128, channels=3, dropout=0.1):\n        super().__init__()\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n        self.patch_size = patch_size\n        self.patch_embed = nn.Linear(patch_dim, dim)\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(dropout)\n        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        p = self.patch_size\n        x = img.unfold(2, p, p).unfold(3, p, p).permute(0, 2, 3, 1, 4, 5).reshape(img.shape[0], -1, 3 * p ** 2)\n        x = self.patch_embed(x)\n        b, n, _ = x.shape\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embed[:, :(n + 1)]\n        x = self.dropout(x)\n        x = self.transformer(x)\n        x = x[:, 0]\n        return self.mlp_head(x)\n\n# Zeta Regularizer (per-parameter for gradients)\ndef zeta_reg(model, lambda_reg=1e-6, epsilon=0.01, N=50):\n    reg = 0.0\n    n_range = torch.arange(1, N+1, dtype=torch.float32, device=device)\n    log_n = torch.log(n_range)\n    pow_n = torch.exp(-(1 + epsilon) * log_n)\n    for p in model.parameters():\n        if p.requires_grad:\n            W = p.view(-1)\n            cos_t = torch.cos(W.unsqueeze(1) * log_n.unsqueeze(0))\n            sin_t = torch.sin(W.unsqueeze(1) * log_n.unsqueeze(0))\n            Re = torch.sum(pow_n.unsqueeze(0) * cos_t, dim=1)\n            Im = -torch.sum(pow_n.unsqueeze(0) * sin_t, dim=1)\n            mag = torch.sqrt(Re**2 + Im**2)\n            reg += lambda_reg * torch.mean(mag)\n    return reg\n\n# Training Function (adapted for ViT)\ndef train_and_eval(use_reg=False, lambda_reg=1e-6, epochs=20, lr=0.001):\n    model = ViT().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    \n    train_losses, train_accs = [], []\n    test_accs = []\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        for data, target in trainloader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            task_loss = criterion(output, target)\n            reg_loss = zeta_reg(model, lambda_reg) if use_reg else 0.0\n            loss = task_loss + reg_loss\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n            total += target.size(0)\n        \n        avg_train_loss = running_loss / len(trainloader)\n        train_acc = 100. * correct / total\n        train_losses.append(avg_train_loss)\n        train_accs.append(train_acc)\n        \n        # Test eval\n        model.eval()\n        test_correct, test_total = 0, 0\n        with torch.no_grad():\n            for data, target in testloader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                pred = output.argmax(dim=1)\n                test_correct += pred.eq(target).sum().item()\n                test_total += target.size(0)\n        test_acc = 100. * test_correct / test_total\n        test_accs.append(test_acc)\n        \n        scheduler.step(avg_train_loss)\n        print(f\"Epoch {epoch+1}/{epochs}: Train Loss {avg_train_loss:.4f}, Train Acc {train_acc:.2f}%, Test Acc {test_acc:.2f}%\")\n    \n    # Sparsity\n    model.eval()\n    all_weights = torch.cat([p.view(-1).abs().cpu() for p in model.parameters()])\n    sparsity = 100 * (all_weights < 0.01).float().mean().item()\n    print(f\"Final Sparsity: {sparsity:.1f}%\")\n    \n    # Plot\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Train Acc')\n    plt.plot(test_accs, label='Test Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.show()\n    \n    return train_losses, train_accs, test_accs, sparsity\n\n# Baseline\nprint(\"=== Baseline ViT (No Reg) ===\")\nbase_train_losses, base_train_accs, base_test_accs, base_sparsity = train_and_eval(use_reg=False)\n\n# ZPSN ViT\nprint(\"\\n=== ZPSN ViT (With Zeta Reg) ===\")\nzpsn_train_losses, zpsn_train_accs, zpsn_test_accs, zpsn_sparsity = train_and_eval(use_reg=True, lambda_reg=1e-6)\n\n# Compare\nprint(f\"\\nComparison:\")\nprint(f\"Baseline ViT: Final Test Acc {base_test_accs[-1]:.2f}%, Sparsity {base_sparsity:.1f}%\")\nprint(f\"ZPSN ViT: Final Test Acc {zpsn_test_accs[-1]:.2f}%, Sparsity {zpsn_sparsity:.1f}%\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:33:44.439105Z","iopub.execute_input":"2025-11-03T14:33:44.43933Z","iopub.status.idle":"2025-11-03T14:55:33.488239Z","shell.execute_reply.started":"2025-11-03T14:33:44.4393Z","shell.execute_reply":"2025-11-03T14:55:33.487457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision.transforms import autoaugment\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hard Task: CIFAR-100 + 1K Subsample + Shift Aug\ntransform_train = transforms.Compose([\n    autoaugment.RandAugment(num_ops=3, magnitude=12),  # Extreme aug\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-100 means/std\n])\ntransform_test_clean = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n# Shifted Test: Jitter + Noise\ntransform_test_shift = transforms.Compose([\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.GaussianBlur(kernel_size=3, sigma=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n\n# Load CIFAR-100 & Extreme Subsample\nfull_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\nindices = np.random.choice(len(full_trainset), 1000, replace=False)  # 1K for overfit hell\ntrainset = Subset(full_trainset, indices)\ntrainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)  # Small batch for low data\n\ntestset_clean = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test_clean)\ntestloader_clean = DataLoader(testset_clean, batch_size=128, shuffle=False, num_workers=2)\n\ntestset_shift = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test_shift)\ntestloader_shift = DataLoader(testset_shift, batch_size=128, shuffle=False, num_workers=2)\n\n# Big Model: ResNet50\nmodel_fn = lambda: models.resnet50(num_classes=100)\n\n# Refined ZPSN (N=200, per-layer norm)\ndef zeta_reg(model, lambda_reg=1e-4, epsilon=0.02, N=200):  # Higher N/λ for limits\n    reg = 0.0\n    n_range = torch.arange(1, N+1, dtype=torch.float32, device=device)\n    log_n = torch.log(n_range)\n    pow_n = torch.exp(-(1 + epsilon) * log_n)\n    for p in model.parameters():\n        if p.requires_grad and p.numel() > 0:\n            W = p.view(-1)\n            num_w = len(W)\n            cos_t = torch.cos(W.unsqueeze(1) * log_n.unsqueeze(0))\n            sin_t = torch.sin(W.unsqueeze(1) * log_n.unsqueeze(0))\n            Re = torch.sum(pow_n.unsqueeze(0) * cos_t, dim=1)\n            Im = -torch.sum(pow_n.unsqueeze(0) * sin_t, dim=1)\n            mag = torch.sqrt(Re**2 + Im**2)\n            reg += torch.mean(mag) * (num_w / 1e6)  # Scale by param size\n    return lambda_reg * reg\n\n# FGSM for Extra Robustness\ndef fgsm_attack(model, data, target, epsilon=0.03):\n    data.requires_grad_(True)\n    output = model(data)\n    loss = F.cross_entropy(output, target)\n    model.zero_grad()\n    loss.backward()\n    pert = epsilon * data.grad.sign()\n    return torch.clamp(data + pert, 0, 1)\n\ndef robust_acc(model, loader, epsilon=0.05, use_shift=False):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            if use_shift:\n                data = fgsm_attack(model, data, target, epsilon)  # Adv on shift\n            output = model(data)\n            correct += output.argmax(1).eq(target).sum().item()\n            total += target.size(0)\n    return 100. * correct / total\n\n# Pruning & Recovery Function\ndef prune_and_recover(model, threshold=0.01, recovery_epochs=5):\n    # Prune: Zero small weights\n    num_pruned = 0\n    for p in model.parameters():\n        mask = (p.abs() < threshold).float()\n        num_pruned += (mask * p.numel()).sum().item()\n        p.data.mul_(1 - mask)\n    \n    # Quick recovery train (low LR)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    model.train()\n    for _ in range(recovery_epochs):\n        for data, target in trainloader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.cross_entropy(output, target)\n            loss.backward()\n            optimizer.step()\n    \n    # Re-eval clean\n    clean_acc = robust_acc(model, testloader_clean)\n    return clean_acc, num_pruned / sum(p.numel() for p in model.parameters() if p.requires_grad) * 100\n\n# Training (Pushed: 100 epochs, adv opt-in)\ndef train_and_eval(use_reg=False, lambda_reg=1e-4, adv_train=False, epochs=100):\n    model = model_fn().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4 if not use_reg else 0)\n    scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=10)  # On acc\n    \n    test_accs_clean, test_accs_shift = [], []\n    for epoch in range(epochs):\n        model.train()\n        running_correct, total = 0, 0\n        for data, target in trainloader:\n            data, target = data.to(device), target.to(device)\n            if adv_train:\n                data = fgsm_attack(model, data, target, 0.02)\n            optimizer.zero_grad()\n            output = model(data)\n            task_loss = criterion(output, target)\n            reg_loss = zeta_reg(model, lambda_reg) if use_reg else 0\n            loss = task_loss + reg_loss\n            loss.backward()\n            optimizer.step()\n            running_correct += output.argmax(1).eq(target).sum().item()\n            total += target.size(0)\n        \n        train_acc = 100. * running_correct / total\n        \n        # Evals\n        acc_clean = robust_acc(model, testloader_clean)\n        acc_shift = robust_acc(model, testloader_shift, use_shift=True)\n        test_accs_clean.append(acc_clean)\n        test_accs_shift.append(acc_shift)\n        \n        scheduler.step(acc_clean)\n        if epoch % 20 == 0:\n            print(f\"Epoch {epoch+1}: Train {train_acc:.1f}%, Clean {acc_clean:.1f}%, Shift {acc_shift:.1f}%\")\n    \n    # Final Metrics\n    sparsity = 100 * (torch.cat([p.abs().flatten() for p in model.parameters() if p.requires_grad]) < 0.01).float().mean()\n    recover_acc, prune_pct = prune_and_recover(model)\n    \n    print(f\"Final: Clean {acc_clean:.2f}%, Shift {acc_shift:.2f}%, Sparsity {sparsity:.1f}%, Prune {prune_pct:.1f}% (Recovers to {recover_acc:.2f}%)\")\n    \n    # Plot\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1); plt.plot(test_accs_clean, label='Clean'); plt.plot(test_accs_shift, label='Shift'); plt.legend(); plt.ylabel('Acc %')\n    plt.subplot(1, 2, 2); plt.plot(test_accs_clean, label='Baseline'); plt.title('Clean Acc Over Epochs'); plt.legend()\n    plt.show()\n    \n    return acc_clean, acc_shift, sparsity, prune_pct, recover_acc\n\n# Experiments to Push Limits\nprint(\"=== Baseline (1K Data, No Reg) ===\")\nbase_clean, base_shift, base_sp, base_prune, base_rec = train_and_eval(use_reg=False)\n\nprint(\"\\n=== ZPSN High λ (1e-4, No Adv) ===\")\nzpsn1_clean, zpsn1_shift, zpsn1_sp, zpsn1_prune, zpsn1_rec = train_and_eval(use_reg=True, lambda_reg=1e-4)\n\nprint(\"\\n=== ZPSN + Adv Train (Pushed Limits) ===\")\nzpsn2_clean, zpsn2_shift, zpsn2_sp, zpsn2_prune, zpsn2_rec = train_and_eval(use_reg=True, lambda_reg=1e-4, adv_train=True)\n\n# Summary\nprint(\"\\n| Variant | Clean Acc | Shift Acc | Sparsity | Prune % | Recover Acc |\")\nprint(\"|---------|-----------|-----------|----------|---------|-------------|\")\nprint(f\"| Baseline | {base_clean:.2f}% | {base_shift:.2f}% | {base_sp:.1f}% | {base_prune:.1f}% | {base_rec:.2f}% |\")\nprint(f\"| ZPSN 1e-4 | {zpsn1_clean:.2f}% | {zpsn1_shift:.2f}% | {zpsn1_sp:.1f}% | {zpsn1_prune:.1f}% | {zpsn1_rec:.2f}% |\")\nprint(f\"| ZPSN+Adv | {zpsn2_clean:.2f}% | {zpsn2_shift:.2f}% | {zpsn2_sp:.1f}% | {zpsn2_prune:.1f}% | {zpsn2_rec:.2f}% |\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}