{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n# ==========================================\n# âš™ï¸ CONFIGURATION\n# ==========================================\nCFG = {\n    # File Paths (Adjust if necessary based on exact input location)\n    'root': '/kaggle/input/cafa-6-protein-function-prediction',\n    'train_fasta': 'Train/train_sequences.fasta',\n    'train_terms': 'Train/train_terms.tsv',\n    'test_fasta':  'Test/testsuperset.fasta',\n    \n    # Model Hyperparameters\n    'num_classes': 600,       # Targets top 600 most frequent terms\n    'max_len': 1024,          # Sequence length clip/pad\n    'embed_dim': 128,         # Embedding size\n    'batch_size': 32,\n    'lr': 1e-3,\n    'epochs': 4,              # Sufficient for convergence on this data\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'seed': 42\n}\n\n# Ensure reproducibility\ntorch.manual_seed(CFG['seed'])\nnp.random.seed(CFG['seed'])\n\n# ==========================================\n# ðŸ§¬ 1. ROBUST DATA LOADING\n# ==========================================\ndef parse_fasta_header(header):\n    \"\"\"\n    Extracts Accession ID from FASTA headers.\n    Handles: '>sp|P12345|NAME' -> 'P12345' AND '>P12345' -> 'P12345'\n    \"\"\"\n    clean = header[1:].strip().split()[0] # Remove '>' and trailing descriptions\n    if clean.count('|') >= 2:\n        return clean.split('|')[1] # Standard UniProt\n    return clean # Fallback\n\ndef load_fasta(path, limit=None):\n    print(f\"ðŸ“‚ Loading sequences from {os.path.basename(path)}...\")\n    full_path = os.path.join(CFG['root'], path)\n    if not os.path.exists(full_path):\n        print(f\"âŒ File not found: {full_path}\")\n        return {}\n    \n    seqs = {}\n    with open(full_path, 'r') as f:\n        sid, seq = \"\", []\n        for i, line in enumerate(f):\n            if line.startswith(\">\"):\n                if sid: seqs[sid] = \"\".join(seq)\n                sid = parse_fasta_header(line)\n                seq = []\n                if limit and len(seqs) >= limit: break\n            else:\n                seq.append(line.strip())\n        if sid and (not limit or len(seqs) < limit): \n            seqs[sid] = \"\".join(seq)\n            \n    print(f\"   -> Loaded {len(seqs):,} sequences.\")\n    return seqs\n\ndef get_training_data():\n    # 1. Load Sequences\n    train_seqs = load_fasta(CFG['train_fasta'])\n    \n    # 2. Load Terms\n    print(\"ðŸ“‚ Loading annotations...\")\n    terms_path = os.path.join(CFG['root'], CFG['train_terms'])\n    # Heuristic for headerless or variable TSV\n    try:\n        df = pd.read_csv(terms_path, sep='\\t')\n        # Standardize columns\n        if df.shape[1] == 2: df.columns = ['EntryID', 'term']\n        elif 'EntryID' not in df.columns: \n            # Assume first two columns are ID and Term\n            df = df.iloc[:, :2]\n            df.columns = ['EntryID', 'term']\n    except:\n        print(\"âŒ Failed to read TSV. Check format.\")\n        return None, None, None, None\n\n    # 3. Filter Classes\n    print(f\"ðŸ“‰ Filtering top {CFG['num_classes']} GO terms...\")\n    top_terms = df['term'].value_counts().index[:CFG['num_classes']].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    df = df[df['term'].isin(top_terms)].copy()\n    df['term_idx'] = df['term'].map(term_map)\n    \n    # 4. Map Annotations to Dictionary\n    print(\"ðŸ”— Mapping Annotations...\")\n    train_annots = df.groupby('EntryID')['term_idx'].apply(set).to_dict()\n    \n    # 5. Intersection (Valid Data)\n    valid_ids = list(set(train_seqs.keys()) & set(train_annots.keys()))\n    print(f\"âœ… Training Intersection: {len(valid_ids):,} proteins.\")\n    \n    return train_seqs, train_annots, valid_ids, top_terms\n\n# ==========================================\n# ðŸ§  2. MODEL ARCHITECTURE (ResNet-1D + SE)\n# ==========================================\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation Block to attend to important channels.\"\"\"\n    def __init__(self, c, r=16):\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool1d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(c, c // r, bias=False),\n            nn.ReLU(),\n            nn.Linear(c // r, c, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _ = x.size()\n        y = self.squeeze(x).view(b, c)\n        y = self.excitation(y).view(b, c, 1)\n        return x * y\n\nclass ResBlock(nn.Module):\n    \"\"\"Residual Block containing Conv1D and SE attention.\"\"\"\n    def __init__(self, in_c, out_c, kernel_size=3, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_c, out_c, kernel_size, stride, padding=kernel_size//2)\n        self.bn1 = nn.BatchNorm1d(out_c)\n        self.conv2 = nn.Conv1d(out_c, out_c, kernel_size, 1, padding=kernel_size//2)\n        self.bn2 = nn.BatchNorm1d(out_c)\n        self.se = SEBlock(out_c)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_c != out_c:\n            self.shortcut = nn.Sequential(\n                nn.Conv1d(in_c, out_c, 1, stride),\n                nn.BatchNorm1d(out_c)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)\n        out += self.shortcut(x)\n        return F.relu(out)\n\nclass ProteinResNet(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.embed = nn.Embedding(22, CFG['embed_dim'], padding_idx=0)\n        \n        # Stem\n        self.conv1 = nn.Conv1d(CFG['embed_dim'], 64, kernel_size=7, stride=1, padding=3)\n        self.bn1 = nn.BatchNorm1d(64)\n        \n        # Backbone (ResNet Stages)\n        self.layer1 = self._make_layer(64, 64, 2)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n        \n        # Head\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def _make_layer(self, in_c, out_c, blocks, stride=1):\n        layers = [ResBlock(in_c, out_c, stride=stride)]\n        for _ in range(1, blocks):\n            layers.append(ResBlock(out_c, out_c))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.embed(x).permute(0, 2, 1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.pool(x).flatten(1)\n        return torch.sigmoid(self.fc(x))\n\n# ==========================================\n# ðŸ” 3. TRAINING LOOP\n# ==========================================\nclass ProteinDataset(Dataset):\n    def __init__(self, seqs, annots, ids):\n        self.seqs = seqs\n        self.annots = annots\n        self.ids = ids\n        self.aa_map = {aa: i+1 for i, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n    \n    def __len__(self): return len(self.ids)\n    \n    def __getitem__(self, idx):\n        pid = self.ids[idx]\n        seq_str = self.seqs[pid]\n        # Tokenize and Pad\n        enc = [self.aa_map.get(aa, 21) for aa in seq_str[:CFG['max_len']]]\n        enc += [0] * (CFG['max_len'] - len(enc))\n        \n        # Label (if training)\n        label = torch.zeros(CFG['num_classes'])\n        if self.annots:\n            for t in self.annots.get(pid, []): label[t] = 1.0\n            \n        return torch.tensor(enc, dtype=torch.long), label, pid\n\ndef train_model():\n    seqs, annots, ids, top_terms = get_training_data()\n    if not ids: return None, None\n    \n    ds = ProteinDataset(seqs, annots, ids)\n    loader = DataLoader(ds, batch_size=CFG['batch_size'], shuffle=True, \n                        num_workers=2, pin_memory=True, drop_last=True)\n    \n    model = ProteinResNet(CFG['num_classes']).to(CFG['device'])\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=1e-4)\n    scaler = GradScaler()\n    criterion = nn.BCELoss()\n    \n    print(\"ðŸš€ Starting Training...\")\n    for epoch in range(CFG['epochs']):\n        model.train()\n        loop = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n        loss_acc = 0\n        \n        for x, y, _ in loop:\n            x, y = x.to(CFG['device']), y.to(CFG['device'])\n            \n            optimizer.zero_grad()\n            with autocast():\n                preds = model(x)\n                loss = criterion(preds, y)\n                \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            loss_acc += loss.item()\n            loop.set_postfix(loss=f\"{loss.item():.4f}\")\n            \n    return model, top_terms\n\n# ==========================================\n# ðŸ“¤ 4. INFERENCE & SUBMISSION\n# ==========================================\ndef inference(model, top_terms):\n    if model is None: return\n    \n    print(\"ðŸ§ª Loading Test Data...\")\n    test_seqs = load_fasta(CFG['test_fasta'])\n    test_ids = list(test_seqs.keys())\n    \n    if not test_ids:\n        print(\"âš ï¸ No test sequences found.\")\n        return\n\n    ds = ProteinDataset(test_seqs, None, test_ids)\n    loader = DataLoader(ds, batch_size=CFG['batch_size']*2, shuffle=False, num_workers=2)\n    \n    model.eval()\n    idx_to_term = {i: t for i, t in enumerate(top_terms)}\n    out_file = 'submission.tsv'\n    \n    print(f\"ðŸ’¾ Running Inference & Saving to {out_file}...\")\n    \n    # We write directly to file to save RAM\n    with open(out_file, 'w') as f:\n        with torch.no_grad():\n            for x, _, pids in tqdm(loader, desc=\"Predicting\"):\n                x = x.to(CFG['device'])\n                preds = model(x).cpu().numpy()\n                \n                # Write High Confidence Predictions\n                batch_out = []\n                for i, pid in enumerate(pids):\n                    scores = preds[i]\n                    # Optimization: Get indices of top 15 terms\n                    top_indices = np.argsort(scores)[-15:][::-1]\n                    \n                    for idx in top_indices:\n                        score = scores[idx]\n                        # Only keep reasonable probabilities to reduce file size\n                        if score > 0.001: \n                            batch_out.append(f\"{pid}\\t{idx_to_term[idx]}\\t{score:.3f}\")\n                            \n                if batch_out:\n                    f.write('\\n'.join(batch_out) + '\\n')\n                    \n    print(\"âœ… Process Complete.\")\n\nif __name__ == \"__main__\":\n    try:\n        model, terms = train_model()\n        inference(model, terms)\n    except Exception as e:\n        print(f\"ðŸ”¥ Critical Error: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}