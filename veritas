{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111808,"databundleVersionId":14485392,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/daltongabrielomondi/veritas?scriptVersionId=282705874\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Veritas AI (Verification of Reasoning Traces & Automated Scoring)\n\n> ```Problem:```\n\nLLMs hallucinate. As they are adopted in high-stakes fields (Finance, Legal, Med), \nblack-box answers are insufficient. Current eval benchmarks are static and leak into training data.\n\n> ```Solution:```\n\nVeritas AI is a \"Reasoning Observability Layer\". We don't just check the answer; \nwe validate the \"Thought Chain\" (as seen in this dataset).\n1. Intercepts LLM calls.\n2. Forces structured CoT (Chain of Thought) output.\n3. Uses a lightweight \"Verifier Model\" (trained on datasets like AI_Thought_Chain) \n   to validate the LOGIC steps, not just the final token.\n\n> ```Monetization:```\n \nB2B API ($.001 per validated thought trace).\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport os\n\n# --- 1. SETUP & DATA LOADING ---\nINPUT_DIR = '/kaggle/input/winners-will-will-exciting-prizes'\n# Look for the dataset file\nfiles = os.listdir(INPUT_DIR)\ndata_file = [f for f in files if 'AI_Thought_Chain' in f][0]\nsample_file = [f for f in files if 'sample_submission' in f][0]\n\ndf = pd.read_csv(f\"{INPUT_DIR}/{data_file}\")\nsub_df = pd.read_csv(f\"{INPUT_DIR}/{sample_file}\")\n\nprint(f\"Dataset Shape: {df.shape}\")\nprint(f\"Sample Submission Columns: {sub_df.columns.tolist()}\")\n\n\ndef solve_question(question):\n    # Pattern: Matches \"What is 10 + 10?\"\n    match = re.search(r'What is (\\d+) \\+ (\\d+)\\?', str(question))\n    if match:\n        n1 = int(match.group(1))\n        n2 = int(match.group(2))\n        return n1 + n2\n    return 0 # Fallback\n\n# Calculate ground truth for the provided dataset\ndf['computed_answer'] = df['question'].apply(solve_question)\n\n\n# Check if sample_submission has an ID column we need to respect\nid_col = sub_df.columns[0] \ntarget_col = sub_df.columns[1] if len(sub_df.columns) > 1 else 'correct_answer'\n\nprint(f\"Detected ID Column: {id_col}\")\nprint(f\"Detected Target Column: {target_col}\")\n\n# We assume the sample submission IDs correspond to the dataset indices (0..999)\nsubmission = pd.DataFrame()\nsubmission[id_col] = sub_df[id_col]\n\n\nif len(submission) == len(df):\n    submission[target_col] = df['computed_answer'].values\nelse:\n    # Fallback if sizes differ (unlikely based on description)\n    print(\"Warning: Size mismatch. Filling with mode.\")\n    submission[target_col] = df['computed_answer'].mode()[0]\n\n\nsubmission[target_col] = submission[target_col].astype(int)\n\n# Save\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"SUCCESS: 'submission.csv' generated.\")\nprint(submission.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:49:21.083578Z","iopub.execute_input":"2025-11-29T18:49:21.083922Z","iopub.status.idle":"2025-11-29T18:49:21.113327Z","shell.execute_reply.started":"2025-11-29T18:49:21.083897Z","shell.execute_reply":"2025-11-29T18:49:21.112356Z"}},"outputs":[{"name":"stdout","text":"Dataset Shape: (1000, 4)\nSample Submission Columns: ['id', 'cot_answer']\nDetected ID Column: id\nDetected Target Column: cot_answer\nWarning: Size mismatch. Filling with mode.\nSUCCESS: 'submission.csv' generated.\n   id  cot_answer\n0   1           2\n1   2           2\n2   3           2\n3   4           2\n4   5           2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}