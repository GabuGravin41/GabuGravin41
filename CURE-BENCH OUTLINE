{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":103432,"databundleVersionId":12925469,"sourceType":"competition"},{"sourceId":228711590,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nCURE-Bench Competition - Advanced Submission System v3.0\nFixed version with proper string literals and pip installs\n\nAuthor: CURE-Bench Advanced System\nVersion: 3.0\n\"\"\"\n\n# ============================================================================\n# CELL 1: Package Installation\n# ============================================================================\n\n# Install required packages first\n!pip install -q openai>=1.0.0\n!pip install -q transformers>=4.36.0\n!pip install -q accelerate>=0.25.0\n!pip install -q bitsandbytes>=0.41.0\n!pip install -q pandas>=2.0.0\n!pip install -q numpy>=1.24.0\n!pip install -q tqdm>=4.65.0\n!pip install -q jsonlines>=3.1.0\n!pip install -q pydantic>=2.0.0\n!pip install -q tenacity>=8.2.0\n!pip install -q sentencepiece  # Required for some tokenizers\n\n# ============================================================================\n# CELL 2: Core Imports\n# ============================================================================\n\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Core imports\nimport json\nimport jsonlines\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple, Any, Union\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport re\nimport time\nfrom datetime import datetime\nimport zipfile\nimport logging\nfrom pathlib import Path\nfrom collections import defaultdict\nimport random\n\n# ML imports\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Try to import quantization support\ntry:\n    from transformers import BitsAndBytesConfig\n    HAS_QUANTIZATION = True\nexcept ImportError:\n    HAS_QUANTIZATION = False\n    print(\"Quantization support not available - will use fp16\")\n\n# Advanced imports\nfrom tqdm import tqdm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom pydantic import BaseModel, Field\n\n# Try to import OpenAI (optional)\ntry:\n    from openai import AzureOpenAI, OpenAI\n    HAS_OPENAI = True\nexcept ImportError:\n    HAS_OPENAI = False\n    print(\"OpenAI not available - will use local models only\")\n\n# ============================================================================\n# CELL 3: Logging Setup\n# ============================================================================\n\n# Create output directory\noutput_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\nlog_dir = os.path.join(output_dir, \"cure_bench_logs\")\nos.makedirs(log_dir, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(os.path.join(log_dir, 'cure_bench.log')),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# ============================================================================\n# CELL 4: Configuration System\n# ============================================================================\n\nclass ModelType(Enum):\n    \"\"\"Available model types\"\"\"\n    RULE_BASED = \"rule_based\"\n    AZURE_GPT4 = \"azure_gpt4\"\n    AZURE_GPT35 = \"azure_gpt35\"\n    OPENAI_GPT4 = \"openai_gpt4\"\n    HF_PIPELINE = \"hf_pipeline\"\n    LOCAL_MISTRAL = \"local_mistral\"\n    LOCAL_LLAMA = \"local_llama\"\n    LOCAL_DEEPSEEK = \"local_deepseek\"\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for individual models\"\"\"\n    name: str\n    type: ModelType\n    enabled: bool = True\n    weight: float = 1.0\n    max_tokens: int = 1000\n    temperature: float = 0.1\n    # API settings\n    api_key: Optional[str] = None\n    endpoint: Optional[str] = None\n    deployment: Optional[str] = None\n    # Local model settings\n    model_id: Optional[str] = None\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    load_in_8bit: bool = False\n    \nclass Config(BaseModel):\n    \"\"\"Main configuration\"\"\"\n    # Model settings\n    use_ensemble: bool = True\n    models: List[ModelConfig] = Field(default_factory=list)\n    \n    # Reasoning enhancements\n    use_chain_of_thought: bool = True\n    use_few_shot: bool = True\n    use_self_consistency: bool = False\n    consistency_samples: int = 3\n    \n    # Processing\n    batch_size: int = 1\n    max_workers: int = 2\n    rate_limit_delay: float = 0.1\n    timeout: int = 60\n    \n    # Paths\n    input_path: str = \"/kaggle/input/cure-bench\"\n    output_path: str = \"/kaggle/working\"\n    test_file: str = \"curebench_testset_phase1.jsonl\"\n    \n    # Debug\n    debug_mode: bool = False\n    sample_size: Optional[int] = None\n    save_intermediate: bool = True\n    \n    class Config:\n        arbitrary_types_allowed = True\n\ndef get_default_config() -> Config:\n    \"\"\"Get default configuration with fallback models\"\"\"\n    config = Config()\n    \n    # Add models in order of preference\n    # 1. DeepSeek R1 Distill (if available in Kaggle)\n    deepseek_path = \"/kaggle/input/deepseek-r1-distill-qwen-7b\"\n    if os.path.exists(deepseek_path):\n        config.models.append(ModelConfig(\n            name=\"deepseek_r1_7b\",\n            type=ModelType.LOCAL_DEEPSEEK,\n            weight=2.5,\n            model_id=deepseek_path,\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n            max_tokens=1500,\n            temperature=0.1,\n            load_in_8bit=True  # Use 8-bit quantization to save memory\n        ))\n    \n    # 2. Azure OpenAI (if available)\n    if HAS_OPENAI and os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n        config.models.append(ModelConfig(\n            name=\"azure_gpt4\",\n            type=ModelType.AZURE_GPT4,\n            weight=2.0,\n            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n            endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n            deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4\")\n        ))\n    \n    # 3. Hugging Face Pipeline (always available)\n    config.models.append(ModelConfig(\n        name=\"hf_medical_qa\",\n        type=ModelType.HF_PIPELINE,\n        weight=1.5,\n        model_id=\"google/flan-t5-base\",  # Lightweight, works on CPU\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ))\n    \n    # 4. Rule-based fallback (always works)\n    config.models.append(ModelConfig(\n        name=\"rule_based\",\n        type=ModelType.RULE_BASED,\n        weight=0.5\n    ))\n    \n    return config\n\n# ============================================================================\n# CELL 5: Base Model Class\n# ============================================================================\n\nclass BaseModel:\n    \"\"\"Base class for all models\"\"\"\n    \n    def __init__(self, config: ModelConfig):\n        self.config = config\n        self.logger = logging.getLogger(f\"{self.__class__.__name__}.{config.name}\")\n        \n    def generate(self, question: str, options: List[str] = None, \n                question_type: str = \"open_ended\") -> Dict[str, Any]:\n        \"\"\"Generate answer with reasoning\"\"\"\n        raise NotImplementedError\n        \n    def format_prompt(self, question: str, options: List[str] = None,\n                     question_type: str = \"open_ended\") -> str:\n        \"\"\"Format prompt for the model\"\"\"\n        prompt = f\"Medical Question: {question}\\n\"\n        \n        if options and question_type in [\"multi_choice\", \"open_ended_multi_choice\"]:\n            prompt += \"\\nOptions:\\n\"\n            for i, option in enumerate(options):\n                prompt += f\"{chr(65+i)}. {option}\\n\"\n        \n        if self.config.type != ModelType.RULE_BASED:\n            prompt += \"\\nProvide your reasoning step by step, then give your final answer.\"\n            prompt += \"\\nFormat: REASONING: [your reasoning] ANSWER: [your answer]\"\n        \n        return prompt\n\n# ============================================================================\n# CELL 6: Rule-Based Model (Always Works)\n# ============================================================================\n\nclass RuleBasedModel(BaseModel):\n    \"\"\"Simple rule-based model as fallback\"\"\"\n    \n    def __init__(self, config: ModelConfig):\n        super().__init__(config)\n        self.medical_keywords = {\n            'safety': ['safe', 'adverse', 'contraindication', 'risk', 'warning'],\n            'efficacy': ['effective', 'efficacy', 'benefit', 'outcome', 'response'],\n            'pediatric': ['child', 'pediatric', 'juvenile', 'young'],\n            'genetic': ['CYP', 'metabolizer', 'genetic', 'polymorphism'],\n            'dosage': ['dose', 'dosage', 'mg', 'administration']\n        }\n        \n    def generate(self, question: str, options: List[str] = None,\n                question_type: str = \"open_ended\") -> Dict[str, Any]:\n        \"\"\"Generate rule-based answer\"\"\"\n        \n        reasoning = self._analyze_question(question, options)\n        answer = self._determine_answer(question, options, reasoning)\n        \n        return {\n            'reasoning': reasoning,\n            'answer': answer,\n            'confidence': 0.3,  # Low confidence for rule-based\n            'model': self.config.name\n        }\n    \n    def _analyze_question(self, question: str, options: List[str]) -> str:\n        \"\"\"Analyze question using rules\"\"\"\n        question_lower = question.lower()\n        reasoning_parts = []\n        \n        # Check for safety concerns\n        if any(kw in question_lower for kw in self.medical_keywords['safety']):\n            reasoning_parts.append(\"This question involves safety considerations.\")\n            \n        # Check for pediatric patient\n        if any(kw in question_lower for kw in self.medical_keywords['pediatric']):\n            reasoning_parts.append(\"The patient is pediatric, requiring special dosing considerations.\")\n            \n        # Check for genetic factors\n        if any(kw in question_lower for kw in self.medical_keywords['genetic']):\n            reasoning_parts.append(\"Genetic factors affect drug metabolism.\")\n            \n        # Analyze options if available\n        if options:\n            safe_options = []\n            for i, opt in enumerate(options):\n                opt_lower = opt.lower()\n                if 'none' in opt_lower or 'avoid' in opt_lower:\n                    reasoning_parts.append(f\"Option {chr(65+i)} suggests avoiding treatment.\")\n                elif any(kw in opt_lower for kw in ['low', 'reduced', 'pediatric']):\n                    safe_options.append(chr(65+i))\n                    reasoning_parts.append(f\"Option {chr(65+i)} appears safer with adjusted dosing.\")\n                    \n        return \" \".join(reasoning_parts) if reasoning_parts else \"Based on general medical principles.\"\n    \n    def _determine_answer(self, question: str, options: List[str], reasoning: str) -> str:\n        \"\"\"Determine answer based on rules\"\"\"\n        if not options:\n            return \"Consult with a healthcare provider for personalized recommendations.\"\n            \n        question_lower = question.lower()\n        \n        # Safety-first approach\n        if 'poor metabolizer' in question_lower:\n            # Look for lowest dose or alternative\n            for i, opt in enumerate(options):\n                if 'low' in opt.lower() or 'reduced' in opt.lower():\n                    return chr(65+i)\n                    \n        # Default to most conservative option\n        if any('none' in opt.lower() for opt in options):\n            for i, opt in enumerate(options):\n                if 'none' in opt.lower():\n                    return chr(65+i)\n                    \n        # Otherwise, choose first non-aspirin option for children\n        if 'child' in question_lower:\n            for i, opt in enumerate(options):\n                if 'aspirin' not in opt.lower():\n                    return chr(65+i)\n                    \n        # Default to first option\n        return \"A\"\n\n# ============================================================================\n# CELL 7: Hugging Face Pipeline Model\n# ============================================================================\n\nclass HFPipelineModel(BaseModel):\n    \"\"\"Hugging Face pipeline model\"\"\"\n    \n    def __init__(self, config: ModelConfig):\n        super().__init__(config)\n        self._initialize_pipeline()\n        \n    def _initialize_pipeline(self):\n        \"\"\"Initialize HF pipeline\"\"\"\n        try:\n            self.logger.info(f\"Loading HF model: {self.config.model_id}\")\n            \n            # For medical QA, we'll use a general model with medical prompting\n            self.pipeline = pipeline(\n                \"text2text-generation\",\n                model=self.config.model_id,\n                device=0 if self.config.device == \"cuda\" and torch.cuda.is_available() else -1,\n                max_length=self.config.max_tokens\n            )\n            \n            self.logger.info(\"HF pipeline initialized successfully\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize HF pipeline: {e}\")\n            raise\n            \n    def generate(self, question: str, options: List[str] = None,\n                question_type: str = \"open_ended\") -> Dict[str, Any]:\n        \"\"\"Generate answer using HF pipeline\"\"\"\n        \n        # Create medical-focused prompt\n        prompt = self._create_medical_prompt(question, options, question_type)\n        \n        try:\n            # Generate response\n            response = self.pipeline(\n                prompt,\n                max_length=self.config.max_tokens,\n                temperature=self.config.temperature,\n                do_sample=True,\n                top_p=0.95\n            )[0]['generated_text']\n            \n            # Parse response\n            reasoning, answer = self._parse_response(response, options)\n            \n            return {\n                'reasoning': reasoning,\n                'answer': answer,\n                'confidence': 0.6,\n                'model': self.config.name\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Generation failed: {e}\")\n            # Fallback to simple answer\n            return {\n                'reasoning': f\"Error in generation: {str(e)}\",\n                'answer': \"A\" if options else \"Unable to determine\",\n                'confidence': 0.1,\n                'model': self.config.name\n            }\n    \n    def _create_medical_prompt(self, question: str, options: List[str], \n                              question_type: str) -> str:\n        \"\"\"Create medical-focused prompt\"\"\"\n        prompt = \"You are a medical expert. Answer the following question:\\n\\n\"\n        prompt += f\"Question: {question}\\n\"\n        \n        if options:\n            prompt += \"\\nOptions:\\n\"\n            for i, opt in enumerate(options):\n                prompt += f\"{chr(65+i)}. {opt}\\n\"\n            prompt += \"\\nSelect the best option and explain why.\\n\"\n        \n        prompt += \"\\nAnswer:\"\n        return prompt\n    \n    def _parse_response(self, response: str, options: List[str]) -> Tuple[str, str]:\n        \"\"\"Parse model response\"\"\"\n        # Simple parsing logic\n        if options:\n            # Look for option letters\n            for i in range(len(options)):\n                letter = chr(65+i)\n                if letter in response.upper():\n                    return response, letter\n                    \n        # Default parsing\n        return response, response[:50] if not options else \"A\"\n\n# ============================================================================\n# CELL 8: Azure OpenAI Model (Optional)\n# ============================================================================\n\nclass AzureOpenAIModel(BaseModel):\n    \"\"\"Azure OpenAI model\"\"\"\n    \n    def __init__(self, config: ModelConfig):\n        super().__init__(config)\n        if not HAS_OPENAI:\n            raise ImportError(\"OpenAI package not available\")\n        self._initialize_client()\n        \n    def _initialize_client(self):\n        \"\"\"Initialize Azure OpenAI client\"\"\"\n        if not self.config.api_key or not self.config.endpoint:\n            raise ValueError(\"Missing Azure OpenAI credentials\")\n            \n        self.client = AzureOpenAI(\n            api_key=self.config.api_key,\n            api_version=\"2024-02-01\",\n            azure_endpoint=self.config.endpoint\n        )\n        self.logger.info(\"Azure OpenAI client initialized\")\n        \n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    def generate(self, question: str, options: List[str] = None,\n                question_type: str = \"open_ended\") -> Dict[str, Any]:\n        \"\"\"Generate using Azure OpenAI\"\"\"\n        \n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert medical AI assistant specializing in drug decision-making. Provide clear reasoning followed by your answer.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": self.format_prompt(question, options, question_type)\n            }\n        ]\n        \n        try:\n            response = self.client.chat.completions.create(\n                model=self.config.deployment,\n                messages=messages,\n                temperature=self.config.temperature,\n                max_tokens=self.config.max_tokens\n            )\n            \n            full_response = response.choices[0].message.content\n            reasoning, answer = self._parse_response(full_response, options)\n            \n            return {\n                'reasoning': reasoning,\n                'answer': answer,\n                'confidence': 0.9,\n                'model': self.config.name\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Azure OpenAI error: {e}\")\n            raise\n            \n    def _parse_response(self, response: str, options: List[str]) -> Tuple[str, str]:\n        \"\"\"Parse Azure OpenAI response\"\"\"\n        if \"REASONING:\" in response and \"ANSWER:\" in response:\n            parts = response.split(\"ANSWER:\")\n            reasoning = parts[0].replace(\"REASONING:\", \"\").strip()\n            answer = parts[1].strip()\n            \n            # Extract letter for multiple choice\n            if options and answer:\n                match = re.search(r'^[(\\[]?([A-Z])[)\\].]?', answer)\n                if match:\n                    answer = match.group(1)\n                    \n            return reasoning, answer\n        else:\n            # Fallback parsing\n            return response, \"A\" if options else response[:100]\n\n# ============================================================================\n# CELL 9: Local DeepSeek Model\n# ============================================================================\n\nclass LocalDeepSeekModel(BaseModel):\n    \"\"\"Local DeepSeek model from Kaggle input\"\"\"\n    \n    def __init__(self, config: ModelConfig):\n        super().__init__(config)\n        self._initialize_model()\n        \n    def _initialize_model(self):\n        \"\"\"Initialize DeepSeek model\"\"\"\n        try:\n            self.logger.info(f\"Loading DeepSeek model from: {self.config.model_id}\")\n            \n            # Load tokenizer\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.config.model_id,\n                trust_remote_code=True,\n                use_fast=True\n            )\n            \n            # Set padding token if needed\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            # Load model with memory optimization\n            if self.config.load_in_8bit and HAS_QUANTIZATION:\n                # Use 8-bit quantization\n                bnb_config = BitsAndBytesConfig(\n                    load_in_8bit=True,\n                    bnb_8bit_compute_dtype=torch.float16\n                )\n                \n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.config.model_id,\n                    trust_remote_code=True,\n                    device_map=\"auto\",\n                    quantization_config=bnb_config,\n                    torch_dtype=torch.float16\n                )\n            else:\n                if self.config.load_in_8bit and not HAS_QUANTIZATION:\n                    self.logger.warning(\"8-bit quantization requested but not available, using fp16\")\n                \n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self.config.model_id,\n                    trust_remote_code=True,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16\n                )\n            \n            self.model.eval()\n            self.logger.info(\"DeepSeek model loaded successfully\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load DeepSeek model: {e}\")\n            raise\n            \n    def generate(self, question: str, options: List[str] = None,\n                question_type: str = \"open_ended\") -> Dict[str, Any]:\n        \"\"\"Generate answer using DeepSeek\"\"\"\n        \n        # Create medical-focused prompt\n        system_prompt = \"\"\"You are an expert medical AI assistant specializing in drug decision-making and precision therapeutics. \nYou have deep knowledge of pharmacology, drug interactions, contraindications, and treatment guidelines.\nWhen answering, always provide clear medical reasoning before giving your final answer.\"\"\"\n\n        prompt = f\"\"\"{system_prompt}\n\n{self.format_prompt(question, options, question_type)}\n\nPlease think step by step and provide your medical reasoning, then give your final answer.\"\"\"\n\n        try:\n            # Tokenize input\n            inputs = self.tokenizer(\n                prompt,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=2048\n            ).to(self.config.device)\n            \n            # Generate with appropriate settings for medical reasoning\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                    do_sample=True,\n                    top_p=0.9,\n                    repetition_penalty=1.1,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode response\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            # Remove the input prompt from response\n            response = response[len(prompt):].strip()\n            \n            # Parse response\n            reasoning, answer = self._parse_response(response, options)\n            \n            # Clean GPU memory\n            del inputs, outputs\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return {\n                'reasoning': reasoning,\n                'answer': answer,\n                'confidence': 0.85,  # High confidence for DeepSeek\n                'model': self.config.name\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"DeepSeek generation error: {e}\")\n            # Return a fallback response\n            return {\n                'reasoning': f\"Error during generation: {str(e)}\",\n                'answer': \"A\" if options else \"Unable to determine\",\n                'confidence': 0.1,\n                'model': self.config.name\n            }\n            \n    def _parse_response(self, response: str, options: List[str]) -> Tuple[str, str]:\n        \"\"\"Parse DeepSeek response\"\"\"\n        # Try multiple parsing strategies\n        \n        # Strategy 1: Look for explicit REASONING/ANSWER format\n        if \"REASONING:\" in response and \"ANSWER:\" in response:\n            parts = response.split(\"ANSWER:\")\n            reasoning = parts[0].replace(\"REASONING:\", \"\").strip()\n            answer = parts[1].strip()\n        # Strategy 2: Look for \"Final answer\" or similar\n        elif any(marker in response.lower() for marker in [\"final answer\", \"therefore\", \"my answer\"]):\n            for marker in [\"final answer:\", \"therefore, the answer is\", \"my answer is\", \"the answer is\"]:\n                if marker.lower() in response.lower():\n                    idx = response.lower().find(marker.lower())\n                    reasoning = response[:idx].strip()\n                    answer = response[idx + len(marker):].strip()\n                    break\n            else:\n                # Split at last sentence\n                sentences = response.split('.')\n                if len(sentences) > 1:\n                    reasoning = '.'.join(sentences[:-1]).strip()\n                    answer = sentences[-1].strip()\n                else:\n                    reasoning = response\n                    answer = \"\"\n        else:\n            # Default: use full response as reasoning\n            reasoning = response\n            answer = \"\"\n        \n        # Extract letter for multiple choice\n        if options:\n            # Look for letter patterns in the answer or end of reasoning\n            text_to_search = answer if answer else reasoning[-100:]\n            \n            # Pattern 1: Explicit letter (A), [A], A., etc.\n            patterns = [\n                r'\\b([A-D])\\b(?:\\)|\\.|\\s|$)',  # Letter followed by ), ., space, or end\n                r'\\[([A-D])\\]',  # [Letter]\n                r'\\(([A-D])\\)',  # (Letter)\n                r'^([A-D])$'     # Just the letter\n            ]\n            \n            for pattern in patterns:\n                match = re.search(pattern, text_to_search.upper())\n                if match:\n                    answer = match.group(1)\n                    break\n            else:\n                # If no pattern found, look for option text\n                for i, opt in enumerate(options):\n                    if opt.lower() in response.lower():\n                        answer = chr(65 + i)\n                        break\n                else:\n                    # Default to A if nothing found\n                    answer = \"A\"\n                    \n        return reasoning, answer\n\n# ============================================================================\n# CELL 10: Model Factory\n# ============================================================================\n\nclass ModelFactory:\n    \"\"\"Factory for creating models\"\"\"\n    \n    @staticmethod\n    def create_model(config: ModelConfig) -> Optional[BaseModel]:\n        \"\"\"Create model based on configuration\"\"\"\n        try:\n            if config.type == ModelType.RULE_BASED:\n                return RuleBasedModel(config)\n            elif config.type == ModelType.HF_PIPELINE:\n                return HFPipelineModel(config)\n            elif config.type == ModelType.LOCAL_DEEPSEEK:\n                return LocalDeepSeekModel(config)\n            elif config.type in [ModelType.AZURE_GPT4, ModelType.AZURE_GPT35]:\n                if HAS_OPENAI:\n                    return AzureOpenAIModel(config)\n                else:\n                    logger.warning(f\"OpenAI not available for {config.name}\")\n                    return None\n            else:\n                logger.warning(f\"Model type {config.type} not implemented\")\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Failed to create model {config.name}: {e}\")\n            return None\n\n# ============================================================================\n# CELL 11: Ensemble System\n# ============================================================================\n\nclass EnsembleSystem:\n    \"\"\"Simple ensemble system\"\"\"\n    \n    def __init__(self, models: List[BaseModel], config: Config):\n        self.models = models\n        self.config = config\n        self.logger = logging.getLogger(\"EnsembleSystem\")\n        \n    def generate_answer(self, question: str, options: List[str] = None,\n                       question_type: str = \"open_ended\") -> Dict[str, Any]:\n        \"\"\"Generate answer using ensemble\"\"\"\n        \n        results = []\n        \n        # Get predictions from all models\n        for model in self.models:\n            try:\n                result = model.generate(question, options, question_type)\n                results.append(result)\n                self.logger.info(f\"Got result from {model.config.name}\")\n            except Exception as e:\n                self.logger.error(f\"Model {model.config.name} failed: {e}\")\n                \n        if not results:\n            # Emergency fallback\n            return {\n                'reasoning': \"All models failed. Using default answer.\",\n                'answer': \"A\" if options else \"Unable to determine\",\n                'confidence': 0.0,\n                'ensemble_details': {'error': 'All models failed'}\n            }\n            \n        # Simple weighted voting\n        final_result = self._weighted_voting(results)\n        final_result['ensemble_details'] = {\n            'num_models': len(results),\n            'model_results': results\n        }\n        \n        return final_result\n        \n    def _weighted_voting(self, results: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Simple weighted voting\"\"\"\n        if len(results) == 1:\n            return results[0]\n            \n        # For multiple choice, vote on answers\n        answer_votes = defaultdict(float)\n        answer_reasoning = defaultdict(list)\n        \n        for result in results:\n            model_name = result.get('model', 'unknown')\n            weight = next((m.config.weight for m in self.models \n                         if m.config.name == model_name), 1.0)\n            confidence = result.get('confidence', 0.5)\n            \n            vote_weight = weight * confidence\n            answer = result['answer']\n            \n            answer_votes[answer] += vote_weight\n            answer_reasoning[answer].append(result['reasoning'])\n            \n        # Get winning answer\n        winning_answer = max(answer_votes.items(), key=lambda x: x[1])[0]\n        \n        # Combine reasoning\n        reasonings = answer_reasoning[winning_answer]\n        if len(reasonings) > 1:\n            combined_reasoning = \"Based on multiple models:\\n\" + \"\\n\".join(\n                f\"- {r[:200]}\" for r in reasonings[:3]\n            )\n        else:\n            combined_reasoning = reasonings[0]\n            \n        # Calculate final confidence\n        total_weight = sum(answer_votes.values())\n        winning_weight = answer_votes[winning_answer]\n        consensus = winning_weight / total_weight if total_weight > 0 else 0\n        \n        return {\n            'reasoning': combined_reasoning,\n            'answer': winning_answer,\n            'confidence': consensus\n        }\n\n# ============================================================================\n# CELL 12: Main Processing Pipeline\n# ============================================================================\n\nclass CUREBenchProcessor:\n    \"\"\"Main processing pipeline\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.logger = logging.getLogger(\"CUREBenchProcessor\")\n        self.models = []\n        self.ensemble = None\n        \n        self._initialize_models()\n        \n    def _initialize_models(self):\n        \"\"\"Initialize all configured models\"\"\"\n        self.logger.info(\"Initializing models...\")\n        \n        for model_config in self.config.models:\n            if not model_config.enabled:\n                continue\n                \n            model = ModelFactory.create_model(model_config)\n            if model:\n                self.models.append(model)\n                self.logger.info(f\"Successfully loaded: {model_config.name}\")\n            else:\n                self.logger.warning(f\"Failed to load: {model_config.name}\")\n                \n        if not self.models:\n            # Ensure we always have at least the rule-based model\n            self.logger.warning(\"No models loaded, adding rule-based fallback\")\n            fallback = ModelFactory.create_model(\n                ModelConfig(name=\"emergency_fallback\", type=ModelType.RULE_BASED)\n            )\n            self.models.append(fallback)\n            \n        # Create ensemble if requested\n        if self.config.use_ensemble and len(self.models) > 1:\n            self.ensemble = EnsembleSystem(self.models, self.config)\n            self.logger.info(f\"Created ensemble with {len(self.models)} models\")\n        else:\n            self.logger.info(f\"Using single model: {self.models[0].config.name}\")\n            \n    def process_dataset(self, input_file: str) -> pd.DataFrame:\n        \"\"\"Process the test dataset\"\"\"\n        self.logger.info(f\"Processing dataset: {input_file}\")\n        \n        # Load questions\n        questions = self._load_questions(input_file)\n        \n        if self.config.debug_mode and self.config.sample_size:\n            questions = questions[:self.config.sample_size]\n            self.logger.info(f\"Debug mode: Processing {len(questions)} questions\")\n            \n        # Process questions\n        results = []\n        for question_data in tqdm(questions, desc=\"Processing questions\"):\n            try:\n                result = self._process_question(question_data)\n                results.append(result)\n                \n                # Rate limiting\n                time.sleep(self.config.rate_limit_delay)\n                \n            except Exception as e:\n                self.logger.error(f\"Failed on question {question_data.get('id')}: {e}\")\n                # Add error result\n                results.append({\n                    'id': question_data.get('id'),\n                    'prediction': 'A',  # Default\n                    'reasoning_trace': f'Processing error: {str(e)}',\n                    'choice': 'A' if question_data.get('options') else ''\n                })\n                \n        # Convert to DataFrame\n        df = pd.DataFrame(results)\n        self.logger.info(f\"Processed {len(df)} questions\")\n        \n        return df\n        \n    def _load_questions(self, filepath: str) -> List[Dict]:\n        \"\"\"Load questions from JSONL\"\"\"\n        questions = []\n        with jsonlines.open(filepath) as reader:\n            for obj in reader:\n                questions.append(obj)\n        return questions\n        \n    def _process_question(self, question_data: Dict) -> Dict:\n        \"\"\"Process single question\"\"\"\n        q_id = question_data.get('id')\n        question = question_data.get('question')\n        q_type = question_data.get('question_type', 'open_ended')\n        options = question_data.get('options', {})\n        \n        # Convert options dict to list\n        if isinstance(options, dict):\n            options_list = [options.get(chr(65+i), '') for i in range(len(options))]\n        else:\n            options_list = options\n            \n        # Generate answer\n        if self.ensemble:\n            result = self.ensemble.generate_answer(question, options_list, q_type)\n        else:\n            result = self.models[0].generate(question, options_list, q_type)\n            \n        # Format for submission\n        return {\n            'id': q_id,\n            'prediction': result['answer'],\n            'reasoning_trace': result['reasoning'],\n            'choice': result['answer'] if options_list else ''\n        }\n\n# ============================================================================\n# CELL 13: Submission Creation\n# ============================================================================\n\ndef create_submission(df: pd.DataFrame, config: Config) -> str:\n    \"\"\"Create submission package\"\"\"\n    logger = logging.getLogger(\"SubmissionCreator\")\n    \n    # Ensure required columns\n    required_columns = ['id', 'prediction', 'reasoning_trace', 'choice']\n    for col in required_columns:\n        if col not in df.columns:\n            df[col] = ''\n            \n    # Save CSV\n    csv_path = os.path.join(config.output_path, \"submission.csv\")\n    df.to_csv(csv_path, index=False)\n    logger.info(f\"Saved submission CSV: {csv_path}\")\n    \n    # Create metadata\n    metadata = {\n        \"meta_data\": {\n            \"model_name\": \"cure_bench_ensemble_v3\",\n            \"track\": \"internal_reasoning\",\n            \"model_type\": \"Ensemble\" if config.use_ensemble else \"Single\",\n            \"base_model_type\": \"Mixed\",\n            \"base_model_name\": f\"{len(config.models)} models\",\n            \"dataset\": \"cure_bench_phase_1\",\n            \"additional_info\": {\n                \"models_used\": [m.name for m in config.models if m.enabled],\n                \"ensemble\": config.use_ensemble\n            }\n        }\n    }\n    \n    metadata_path = os.path.join(config.output_path, \"metadata.json\")\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n        \n    # Create zip\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    zip_path = os.path.join(config.output_path, f\"cure_bench_submission_{timestamp}.zip\")\n    \n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        zipf.write(csv_path, \"submission.csv\")\n        zipf.write(metadata_path, \"metadata.json\")\n        \n    logger.info(f\"Created submission: {zip_path}\")\n    return zip_path\n\n# ============================================================================\n# CELL 14: Main Execution\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"CURE-Bench Submission System v3.0\")\n    print(\"=\"*60)\n    \n    # Get configuration\n    config = get_default_config()\n    \n    # Check for DeepSeek model\n    deepseek_available = any(m.type == ModelType.LOCAL_DEEPSEEK for m in config.models)\n    if deepseek_available:\n        print(\"\\n‚úÖ DeepSeek R1 Distill model detected!\")\n    \n    # Print configuration\n    print(\"\\nConfiguration:\")\n    print(f\"- Models: {len([m for m in config.models if m.enabled])} enabled\")\n    for model in config.models:\n        if model.enabled:\n            icon = \"üöÄ\" if model.type == ModelType.LOCAL_DEEPSEEK else \"‚Ä¢\"\n            print(f\"  {icon} {model.name} ({model.type.value})\")\n    print(f\"- Ensemble: {config.use_ensemble}\")\n    print(f\"- Debug mode: {config.debug_mode}\")\n    \n    # Check for input file\n    input_file = os.path.join(config.input_path, config.test_file)\n    if not os.path.exists(input_file):\n        print(f\"\\n‚ùå ERROR: Input file not found: {input_file}\")\n        print(\"Please ensure the dataset is in the input directory\")\n        return\n        \n    # Create processor\n    print(\"\\nüì• Initializing processor...\")\n    processor = CUREBenchProcessor(config)\n    \n    # Process dataset\n    print(f\"\\nüî¨ Processing dataset...\")\n    start_time = time.time()\n    \n    try:\n        df = processor.process_dataset(input_file)\n        \n        # Create submission\n        print(f\"\\nüì¶ Creating submission...\")\n        submission_path = create_submission(df, config)\n        \n        # Summary\n        elapsed = time.time() - start_time\n        print(f\"\\n‚úÖ COMPLETE!\")\n        print(f\"- Processed {len(df)} questions\")\n        print(f\"- Time: {elapsed:.1f} seconds\")\n        print(f\"- Output: {submission_path}\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå ERROR: {e}\")\n        logger.exception(\"Fatal error\")\n        \n# ============================================================================\n# CELL 15: Quick Start Functions\n# ============================================================================\n\ndef quick_test(n_samples: int = 5):\n    \"\"\"Quick test with n samples\"\"\"\n    global config\n    config = get_default_config()\n    config.debug_mode = True\n    config.sample_size = n_samples\n    config.use_ensemble = False  # Faster\n    print(f\"üöÄ Running quick test with {n_samples} samples...\")\n    \n    # Update global config before running main\n    processor = CUREBenchProcessor(config)\n    \n    # Process with limited samples\n    input_file = os.path.join(config.input_path, config.test_file)\n    if os.path.exists(input_file):\n        df = processor.process_dataset(input_file)\n        submission_path = create_submission(df, config)\n        print(f\"‚úÖ Test complete! Output: {submission_path}\")\n    else:\n        print(f\"‚ùå Input file not found: {input_file}\")\n\ndef full_run():\n    \"\"\"Full competition run\"\"\"\n    global config\n    config = get_default_config()\n    config.debug_mode = False\n    config.sample_size = None\n    print(\"üéØ Running full submission...\")\n    main()\n\n# ============================================================================\n# EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    # Auto-detect environment\n    if os.path.exists('/kaggle/working'):\n        print(\"üéØ Detected Kaggle environment\")\n    else:\n        print(\"üíª Running locally\")\n        \n    print(\"\\nUsage:\")\n    print(\"- quick_test(5)  # Test with 5 samples\")\n    print(\"- full_run()     # Full submission\")\n    print(\"- main()         # Run with current config\")\n    \n    # Default: run full submission\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}