{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635e26a",
   "metadata": {
    "_cell_guid": "0ec49051-b8ac-4f27-b152-0c2fe59d4b5c",
    "_uuid": "7e2d63ef-4429-4722-95ee-6681af62a6d5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T19:10:50.944391Z",
     "iopub.status.busy": "2025-11-29T19:10:50.943867Z",
     "iopub.status.idle": "2025-11-29T19:10:54.060874Z",
     "shell.execute_reply": "2025-11-29T19:10:54.059493Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.127101,
     "end_time": "2025-11-29T19:10:54.062993",
     "exception": false,
     "start_time": "2025-11-29T19:10:50.935892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data files...\n",
      "Loaded 844 training sequences, 12 validation sequences, and 12 test sequences\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nLoading data files...\")\n",
    "train_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv')\n",
    "valid_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv')\n",
    "test_seqs = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/test_sequences.csv')\n",
    "train_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_labels.csv')\n",
    "valid_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_labels.csv')\n",
    "\n",
    "print(f\"Loaded {len(train_seqs)} training sequences, {len(valid_seqs)} validation sequences, and {len(test_seqs)} test sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259042a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:10:54.077032Z",
     "iopub.status.busy": "2025-11-29T19:10:54.076630Z",
     "iopub.status.idle": "2025-11-29T19:10:54.081307Z",
     "shell.execute_reply": "2025-11-29T19:10:54.079673Z"
    },
    "papermill": {
     "duration": 0.014219,
     "end_time": "2025-11-29T19:10:54.083706",
     "exception": false,
     "start_time": "2025-11-29T19:10:54.069487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_seqs_v2 = pd.read_csv('/kaggle/input/extended-rna/train_sequences_v2.csv')\n",
    "# train_labels_v2 = pd.read_csv('/kaggle/input/extended-rna/train_labels_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde10c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:10:54.097185Z",
     "iopub.status.busy": "2025-11-29T19:10:54.096547Z",
     "iopub.status.idle": "2025-11-29T19:11:07.412001Z",
     "shell.execute_reply": "2025-11-29T19:11:07.410852Z"
    },
    "papermill": {
     "duration": 13.324856,
     "end_time": "2025-11-29T19:11:07.414416",
     "exception": false,
     "start_time": "2025-11-29T19:10:54.089560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_seqs_v2 = pd.read_csv('/kaggle/input/rna-cif-to-csv/rna_sequences.csv')\n",
    "train_labels_v2 = pd.read_csv('/kaggle/input/rna-cif-to-csv/rna_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19eefcd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:11:07.427367Z",
     "iopub.status.busy": "2025-11-29T19:11:07.427004Z",
     "iopub.status.idle": "2025-11-29T19:11:07.453110Z",
     "shell.execute_reply": "2025-11-29T19:11:07.451986Z"
    },
    "papermill": {
     "duration": 0.034537,
     "end_time": "2025-11-29T19:11:07.454991",
     "exception": false,
     "start_time": "2025-11-29T19:11:07.420454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2D19_A</td>\n",
       "      <td>GCUGAAGUGCACACGGC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6OXI_QA</td>\n",
       "      <td>GUUGGAGAGUUUGAUCCUGGCUCAGGGUGAACGCUGGCGGCGUGCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6OXI_QV</td>\n",
       "      <td>CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6OXI_QX</td>\n",
       "      <td>CAAGGAGGUAAAAAUGU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6OXI_RA</td>\n",
       "      <td>AGAUGGUAAGGGCCCACGGUGGAUGCCUCGGCACCCGAGCCGAUGA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target_id                                           sequence\n",
       "0    2D19_A                                  GCUGAAGUGCACACGGC\n",
       "1   6OXI_QA  GUUGGAGAGUUUGAUCCUGGCUCAGGGUGAACGCUGGCGGCGUGCC...\n",
       "2   6OXI_QV  CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAG...\n",
       "3   6OXI_QX                                  CAAGGAGGUAAAAAUGU\n",
       "4   6OXI_RA  AGAUGGUAAGGGCCCACGGUGGAUGCCUCGGCACCCGAGCCGAUGA..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880489b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:11:07.468758Z",
     "iopub.status.busy": "2025-11-29T19:11:07.468312Z",
     "iopub.status.idle": "2025-11-29T19:11:07.501605Z",
     "shell.execute_reply": "2025-11-29T19:11:07.500270Z"
    },
    "papermill": {
     "duration": 0.042824,
     "end_time": "2025-11-29T19:11:07.503852",
     "exception": false,
     "start_time": "2025-11-29T19:11:07.461028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10135546 entries, 0 to 10135545\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   ID       object \n",
      " 1   resname  object \n",
      " 2   resid    int64  \n",
      " 3   x_1      float64\n",
      " 4   y_1      float64\n",
      " 5   z_1      float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 464.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_labels_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd57fec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:11:07.518085Z",
     "iopub.status.busy": "2025-11-29T19:11:07.517662Z",
     "iopub.status.idle": "2025-11-29T19:11:07.525188Z",
     "shell.execute_reply": "2025-11-29T19:11:07.523355Z"
    },
    "papermill": {
     "duration": 0.017325,
     "end_time": "2025-11-29T19:11:07.527742",
     "exception": false,
     "start_time": "2025-11-29T19:11:07.510417",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. Check if v2 is larger than original\n",
    "# print(\"Dataset Size Comparison:\")\n",
    "# print(f\"train_seqs: {len(train_seqs)} rows → train_seqs_v2: {len(train_seqs_v2)} rows\")\n",
    "# print(f\"train_labels: {len(train_labels)} rows → train_labels_v2: {len(train_labels_v2)} rows\")\n",
    "# print()\n",
    "\n",
    "# # 2. Verify all train_seqs records exist in train_seqs_v2\n",
    "# print(\"Checking if original sequences exist in v2...\")\n",
    "# original_target_ids = set(train_seqs['target_id'])\n",
    "# extended_target_ids = set(train_seqs_v2['target_id'])\n",
    "# all_targets_included = original_target_ids.issubset(extended_target_ids)\n",
    "\n",
    "# print(f\"Original train_seqs has {len(original_target_ids)} unique target_ids\")\n",
    "# print(f\"All original target_ids found in train_seqs_v2: {all_targets_included}\")\n",
    "\n",
    "# if not all_targets_included:\n",
    "#     missing_ids = original_target_ids - extended_target_ids\n",
    "#     print(f\"Missing {len(missing_ids)} target_ids\")\n",
    "#     if len(missing_ids) <= 5:\n",
    "#         print(f\"Missing IDs: {list(missing_ids)}\")\n",
    "#     else:\n",
    "#         print(f\"First 5 missing IDs: {list(missing_ids)[:5]}\")\n",
    "# print()\n",
    "\n",
    "# # 3. Check for consistency in a sample of target_ids that exist in both datasets\n",
    "# if all_targets_included:\n",
    "#     print(\"Checking consistency of data between original and v2 sequences...\")\n",
    "#     # Sample a few target_ids that exist in both datasets\n",
    "#     sample_size = min(5, len(original_target_ids))\n",
    "#     sample_ids = np.random.choice(list(original_target_ids), sample_size, replace=False)\n",
    "    \n",
    "#     for target_id in sample_ids:\n",
    "#         original_row = train_seqs[train_seqs['target_id'] == target_id].iloc[0]\n",
    "#         extended_row = train_seqs_v2[train_seqs_v2['target_id'] == target_id].iloc[0]\n",
    "        \n",
    "#         # Compare important columns\n",
    "#         sequence_match = original_row['sequence'] == extended_row['sequence']\n",
    "#         print(f\"Target ID {target_id}: Sequences match: {sequence_match}\")\n",
    "# print()\n",
    "\n",
    "# # 4. Check if all train_labels IDs exist in train_labels_v2\n",
    "# print(\"Checking labels dataset...\")\n",
    "# # Since labels dataset is large, we'll check a sample of IDs\n",
    "# sample_size = min(1000, len(train_labels))\n",
    "# sample_indices = np.random.choice(len(train_labels), sample_size, replace=False)\n",
    "# sample_rows = train_labels.iloc[sample_indices]\n",
    "\n",
    "# # Create a composite key for comparison (ID + resid)\n",
    "# sample_rows['composite_key'] = sample_rows['ID'] + '_' + sample_rows['resid'].astype(str)\n",
    "# train_labels_v2['composite_key'] = train_labels_v2['ID'] + '_' + train_labels_v2['resid'].astype(str)\n",
    "\n",
    "# sample_keys = set(sample_rows['composite_key'])\n",
    "# extended_keys = set(train_labels_v2['composite_key'])\n",
    "\n",
    "# keys_found = sample_keys.issubset(extended_keys)\n",
    "# if keys_found:\n",
    "#     found_percentage = 100\n",
    "# else:\n",
    "#     intersection = sample_keys.intersection(extended_keys)\n",
    "#     found_percentage = (len(intersection) / len(sample_keys)) * 100\n",
    "\n",
    "# print(f\"Sampled {len(sample_keys)} keys from train_labels\")\n",
    "# print(f\"All sampled keys found in train_labels_v2: {keys_found} ({found_percentage:.2f}%)\")\n",
    "\n",
    "# if not keys_found:\n",
    "#     missing_keys = sample_keys - extended_keys\n",
    "#     print(f\"Missing {len(missing_keys)} keys out of {len(sample_keys)} sampled\")\n",
    "#     if len(missing_keys) <= 5:\n",
    "#         print(f\"Missing keys: {list(missing_keys)}\")\n",
    "#     else:\n",
    "#         print(f\"First 5 missing keys: {list(missing_keys)[:5]}\")\n",
    "# print()\n",
    "\n",
    "# # 5. Data type consistency check\n",
    "# print(\"Data type consistency check:\")\n",
    "# print(\"train_seqs vs train_seqs_v2:\")\n",
    "# for col in train_seqs.columns:\n",
    "#     print(f\"  Column '{col}': {train_seqs[col].dtype} → {train_seqs_v2[col].dtype} - Match: {train_seqs[col].dtype == train_seqs_v2[col].dtype}\")\n",
    "\n",
    "# print(\"\\ntrain_labels vs train_labels_v2:\")\n",
    "# for col in train_labels.columns:\n",
    "#     if col != 'composite_key':  # Skip the key we created\n",
    "#         print(f\"  Column '{col}': {train_labels[col].dtype} → {train_labels_v2[col].dtype} - Match: {train_labels[col].dtype == train_labels_v2[col].dtype}\")\n",
    "# print()\n",
    "\n",
    "# # 6. Check for missing values pattern\n",
    "# print(\"Missing values comparison:\")\n",
    "# print(\"train_seqs vs train_seqs_v2:\")\n",
    "# for col in train_seqs.columns:\n",
    "#     original_missing = train_seqs[col].isnull().sum() / len(train_seqs) * 100\n",
    "#     extended_missing = train_seqs_v2[col].isnull().sum() / len(train_seqs_v2) * 100\n",
    "#     print(f\"  Column '{col}': {original_missing:.2f}% → {extended_missing:.2f}%\")\n",
    "\n",
    "# print(\"\\ntrain_labels vs train_labels_v2:\")\n",
    "# for col in train_labels.columns:\n",
    "#     if col != 'composite_key':  # Skip the key we created\n",
    "#         original_missing = train_labels[col].isnull().sum() / len(train_labels) * 100\n",
    "#         extended_missing = train_labels_v2[col].isnull().sum() / len(train_labels_v2) * 100\n",
    "#         print(f\"  Column '{col}': {original_missing:.2f}% → {extended_missing:.2f}%\")\n",
    "# print()\n",
    "\n",
    "# # Clean up the temporary column we added\n",
    "# if 'composite_key' in train_labels_v2.columns:\n",
    "#     train_labels_v2.drop('composite_key', axis=1, inplace=True)\n",
    "\n",
    "# # Final assessment\n",
    "# print(\"FINAL ASSESSMENT:\")\n",
    "# print(\"-\" * 50)\n",
    "# seqs_extended_properly = all_targets_included and len(train_seqs_v2) > len(train_seqs)\n",
    "# labels_extended_properly = found_percentage > 99 and len(train_labels_v2) > len(train_labels)\n",
    "\n",
    "# if seqs_extended_properly and labels_extended_properly:\n",
    "#     print(\"✓ PASS: Both train_seqs_v2 and train_labels_v2 appear to be proper extensions of the original datasets.\")\n",
    "#     print(\"✓ It should be safe to swap them.\")\n",
    "# else:\n",
    "#     print(\"✗ ISSUES DETECTED:\")\n",
    "#     if not seqs_extended_properly:\n",
    "#         print(\"  - train_seqs_v2 may not fully contain train_seqs data\")\n",
    "#     if not labels_extended_properly:\n",
    "#         print(\"  - train_labels_v2 may not fully contain train_labels data\")\n",
    "#     print(\"✗ Recommend investigating the issues above before swapping datasets.\")\n",
    "# print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49855cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:11:07.543617Z",
     "iopub.status.busy": "2025-11-29T19:11:07.543246Z",
     "iopub.status.idle": "2025-11-29T19:12:37.532820Z",
     "shell.execute_reply": "2025-11-29T19:12:37.531390Z"
    },
    "papermill": {
     "duration": 89.999565,
     "end_time": "2025-11-29T19:12:37.535260",
     "exception": false,
     "start_time": "2025-11-29T19:11:07.535695",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTENDING SEQUENCE DATASETS\n",
      "==================================================\n",
      "Extending train_seqs...\n",
      "  Original size: 844 rows\n",
      "  v2 size: 18881 rows\n",
      "  Keys only in original: 65\n",
      "  Keys only in v2: 18102\n",
      "  Common keys: 779\n",
      "  New records added: 18102\n",
      "  Extended dataset size: 18946 rows\n",
      "  Verification - All original keys in extended dataset: True\n",
      "  Column 'temporal_cutoff': Missing values - Original: 0, Extended: 18102\n",
      "  Column 'description': Missing values - Original: 0, Extended: 18102\n",
      "  Column 'all_sequences': Missing values - Original: 5, Extended: 18107\n",
      "\n",
      "==================================================\n",
      "EXTENDING LABELS DATASETS\n",
      "==================================================\n",
      "Extending train_labels...\n",
      "  Original size: 137095 rows\n",
      "  v2 size: 10135546 rows\n",
      "  Keys only in original: 11896\n",
      "  Keys only in v2: 10010347\n",
      "  Common keys: 125199\n",
      "  New records added: 10010347\n",
      "  Extended dataset size: 10147442 rows\n",
      "  Verification - All original keys in extended dataset: True\n",
      "  Column 'x_1': Missing values - Original: 6145, Extended: 6145\n",
      "  Column 'y_1': Missing values - Original: 6145, Extended: 6145\n",
      "  Column 'z_1': Missing values - Original: 6145, Extended: 6145\n",
      "\n",
      "==================================================\n",
      "VERIFYING RELATIONSHIPS\n",
      "==================================================\n",
      "Total unique sequence IDs: 18946\n",
      "Sequence IDs with corresponding labels: 0 (0.00%)\n",
      "Sequence IDs without corresponding labels: 18946 (100.00%)\n",
      "Sample of sequence IDs without labels (up to 5):\n",
      "['4YZV_RB', '7JT1_1', '6Q8Y_BQ', '8FTO_A', '5AFI_y']\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF EXTENDED DATASETS\n",
      "==================================================\n",
      "Original train_seqs: 844 rows\n",
      "Original train_labels: 137095 rows\n",
      "Extended train_seqs: 18946 rows (+18102)\n",
      "Extended train_labels: 10147442 rows (+10010347)\n",
      "\n",
      "==================================================\n",
      "DONE! Extended datasets created.\n",
      "To save the datasets, uncomment the last two lines.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to extend the original dataset with new records from v2\n",
    "def extend_dataset(original_df, v2_df, key_columns, dataset_name):\n",
    "    print(f\"Extending {dataset_name}...\")\n",
    "    print(f\"  Original size: {len(original_df)} rows\")\n",
    "    print(f\"  v2 size: {len(v2_df)} rows\")\n",
    "    \n",
    "    # Create a composite key for identification if multiple key columns\n",
    "    if isinstance(key_columns, list) and len(key_columns) > 1:\n",
    "        original_df['temp_key'] = original_df[key_columns].astype(str).agg('_'.join, axis=1)\n",
    "        v2_df['temp_key'] = v2_df[key_columns].astype(str).agg('_'.join, axis=1)\n",
    "        key_for_identification = 'temp_key'\n",
    "    else:\n",
    "        key_for_identification = key_columns[0] if isinstance(key_columns, list) else key_columns\n",
    "    \n",
    "    # Identify unique records in each dataset\n",
    "    original_keys = set(original_df[key_for_identification])\n",
    "    v2_keys = set(v2_df[key_for_identification])\n",
    "    \n",
    "    # Calculate stats\n",
    "    keys_only_in_original = original_keys - v2_keys\n",
    "    keys_only_in_v2 = v2_keys - original_keys \n",
    "    common_keys = original_keys.intersection(v2_keys)\n",
    "    \n",
    "    print(f\"  Keys only in original: {len(keys_only_in_original)}\")\n",
    "    print(f\"  Keys only in v2: {len(keys_only_in_v2)}\")\n",
    "    print(f\"  Common keys: {len(common_keys)}\")\n",
    "    \n",
    "    # Create a mask to filter v2 records that don't exist in original\n",
    "    new_records_mask = ~v2_df[key_for_identification].isin(original_keys)\n",
    "    new_records = v2_df[new_records_mask].copy()\n",
    "    \n",
    "    # Drop temporary key if it was created\n",
    "    if key_for_identification == 'temp_key':\n",
    "        new_records.drop('temp_key', axis=1, inplace=True)\n",
    "        original_df.drop('temp_key', axis=1, inplace=True)\n",
    "    \n",
    "    # Combine original with new records from v2\n",
    "    extended_df = pd.concat([original_df, new_records], ignore_index=True)\n",
    "    \n",
    "    # Report final sizes\n",
    "    print(f\"  New records added: {len(new_records)}\")\n",
    "    print(f\"  Extended dataset size: {len(extended_df)} rows\")\n",
    "    print(f\"  Verification - All original keys in extended dataset: {set(original_df[key_columns[0] if isinstance(key_columns, list) else key_columns]).issubset(set(extended_df[key_columns[0] if isinstance(key_columns, list) else key_columns]))}\")\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    for col in extended_df.columns:\n",
    "        original_missing = original_df[col].isnull().sum()\n",
    "        extended_missing = extended_df[col].isnull().sum()\n",
    "        if original_missing > 0 or extended_missing > 0:\n",
    "            print(f\"  Column '{col}': Missing values - Original: {original_missing}, Extended: {extended_missing}\")\n",
    "    \n",
    "    # Clean up\n",
    "    if key_for_identification == 'temp_key' and 'temp_key' in v2_df.columns:\n",
    "        v2_df.drop('temp_key', axis=1, inplace=True)\n",
    "        \n",
    "    return extended_df\n",
    "\n",
    "# 1. Extend train_seqs with train_seqs_v2\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDING SEQUENCE DATASETS\")\n",
    "print(\"=\"*50)\n",
    "train_seqs_extended = extend_dataset(\n",
    "    train_seqs, \n",
    "    train_seqs_v2,\n",
    "    ['target_id'],  # Using target_id as the unique identifier\n",
    "    \"train_seqs\"\n",
    ")\n",
    "\n",
    "# 2. Extend train_labels with train_labels_v2\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDING LABELS DATASETS\")\n",
    "print(\"=\"*50)\n",
    "# For labels, we need a composite key of ID and resid\n",
    "train_labels_extended = extend_dataset(\n",
    "    train_labels,\n",
    "    train_labels_v2,\n",
    "    ['ID', 'resid'],  # Using composite key\n",
    "    \"train_labels\"\n",
    ")\n",
    "\n",
    "# Verify relationships between extended datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFYING RELATIONSHIPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if all sequence IDs have corresponding labels\n",
    "seq_ids = set(train_seqs_extended['target_id'].unique())\n",
    "label_ids = set(train_labels_extended['ID'].unique())\n",
    "\n",
    "seq_ids_with_labels = seq_ids.intersection(label_ids)\n",
    "seq_ids_without_labels = seq_ids - label_ids\n",
    "\n",
    "print(f\"Total unique sequence IDs: {len(seq_ids)}\")\n",
    "print(f\"Sequence IDs with corresponding labels: {len(seq_ids_with_labels)} ({len(seq_ids_with_labels)/len(seq_ids)*100:.2f}%)\")\n",
    "print(f\"Sequence IDs without corresponding labels: {len(seq_ids_without_labels)} ({len(seq_ids_without_labels)/len(seq_ids)*100:.2f}%)\")\n",
    "\n",
    "if len(seq_ids_without_labels) > 0:\n",
    "    print(\"Sample of sequence IDs without labels (up to 5):\")\n",
    "    print(list(seq_ids_without_labels)[:5])\n",
    "\n",
    "# Print summary of extended datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY OF EXTENDED DATASETS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original train_seqs: {len(train_seqs)} rows\")\n",
    "print(f\"Original train_labels: {len(train_labels)} rows\")\n",
    "print(f\"Extended train_seqs: {len(train_seqs_extended)} rows (+{len(train_seqs_extended)-len(train_seqs)})\")\n",
    "print(f\"Extended train_labels: {len(train_labels_extended)} rows (+{len(train_labels_extended)-len(train_labels)})\")\n",
    "\n",
    "# Save the extended datasets (uncomment to save)\n",
    "# train_seqs_extended.to_csv('train_seqs_combined.csv', index=False)\n",
    "# train_labels_extended.to_csv('train_labels_combined.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DONE! Extended datasets created.\")\n",
    "print(\"To save the datasets, uncomment the last two lines.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbc2b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:12:37.550069Z",
     "iopub.status.busy": "2025-11-29T19:12:37.549650Z",
     "iopub.status.idle": "2025-11-29T19:12:37.565048Z",
     "shell.execute_reply": "2025-11-29T19:12:37.563700Z"
    },
    "papermill": {
     "duration": 0.025151,
     "end_time": "2025-11-29T19:12:37.567314",
     "exception": false,
     "start_time": "2025-11-29T19:12:37.542163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18946 entries, 0 to 18945\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   target_id        18946 non-null  object\n",
      " 1   sequence         18946 non-null  object\n",
      " 2   temporal_cutoff  844 non-null    object\n",
      " 3   description      844 non-null    object\n",
      " 4   all_sequences    839 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 740.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_seqs_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec0772f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:12:37.581639Z",
     "iopub.status.busy": "2025-11-29T19:12:37.581295Z",
     "iopub.status.idle": "2025-11-29T19:12:37.590391Z",
     "shell.execute_reply": "2025-11-29T19:12:37.589077Z"
    },
    "papermill": {
     "duration": 0.018302,
     "end_time": "2025-11-29T19:12:37.592314",
     "exception": false,
     "start_time": "2025-11-29T19:12:37.574012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10147442 entries, 0 to 10147441\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   ID       object \n",
      " 1   resname  object \n",
      " 2   resid    int64  \n",
      " 3   x_1      float64\n",
      " 4   y_1      float64\n",
      " 5   z_1      float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 464.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_labels_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bde1430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:12:37.607380Z",
     "iopub.status.busy": "2025-11-29T19:12:37.606995Z",
     "iopub.status.idle": "2025-11-29T19:12:37.611165Z",
     "shell.execute_reply": "2025-11-29T19:12:37.610043Z"
    },
    "papermill": {
     "duration": 0.013963,
     "end_time": "2025-11-29T19:12:37.613044",
     "exception": false,
     "start_time": "2025-11-29T19:12:37.599081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the first 500 sequences\n",
    "# train_seqs_small = train_seqs_extended.iloc[:500].copy()\n",
    "\n",
    "# # Extract base IDs from train_labels_extended once\n",
    "# base_ids = train_labels_extended['ID'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Filter labels where the base ID is in our sequence IDs\n",
    "# train_labels_small = train_labels_extended[base_ids.isin(train_seqs_small['target_id'])].copy()\n",
    "\n",
    "# # Verify\n",
    "# print(f\"Number of sequences in train_seqs_small: {len(train_seqs_small)}\")\n",
    "# print(f\"Total rows in train_labels_small: {len(train_labels_small)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cb18b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:12:37.627841Z",
     "iopub.status.busy": "2025-11-29T19:12:37.627513Z",
     "iopub.status.idle": "2025-11-29T19:23:48.394244Z",
     "shell.execute_reply": "2025-11-29T19:23:48.392643Z"
    },
    "papermill": {
     "duration": 670.776839,
     "end_time": "2025-11-29T19:23:48.396540",
     "exception": false,
     "start_time": "2025-11-29T19:12:37.619701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing structures: 100%|██████████| 18815/18815 [10:03<00:00, 31.17it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    \n",
    "    # Group by target ID and wrap with tqdm for progress tracking\n",
    "    id_groups = labels_df.groupby(lambda x: labels_df['ID'][x].rsplit('_', 1)[0])\n",
    "    for id_prefix, group in tqdm(id_groups, desc=\"Processing structures\"):\n",
    "        # Extract just the coordinates columns for the first structure (x_1, y_1, z_1)\n",
    "        coords = []\n",
    "        for _, row in group.sort_values('resid').iterrows():\n",
    "            coords.append([row['x_1'], row['y_1'], row['z_1']])\n",
    "        \n",
    "        coords_dict[id_prefix] = np.array(coords)\n",
    "    \n",
    "    return coords_dict\n",
    "\n",
    "train_coords_dict = process_labels(train_labels_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a644a",
   "metadata": {
    "papermill": {
     "duration": 0.156096,
     "end_time": "2025-11-29T19:23:48.711991",
     "exception": false,
     "start_time": "2025-11-29T19:23:48.555895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33438596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:49.021668Z",
     "iopub.status.busy": "2025-11-29T19:23:49.021312Z",
     "iopub.status.idle": "2025-11-29T19:23:49.679399Z",
     "shell.execute_reply": "2025-11-29T19:23:49.678189Z"
    },
    "papermill": {
     "duration": 0.818373,
     "end_time": "2025-11-29T19:23:49.681996",
     "exception": false,
     "start_time": "2025-11-29T19:23:48.863623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import pairwise2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n",
    "    \"\"\"\n",
    "    Find similar RNA sequences using enhanced scoring and clustering for diversity.\n",
    "    \n",
    "    Improvements:\n",
    "    - Multi-tier length filtering\n",
    "    - Enhanced alignment scoring with multiple algorithms\n",
    "    - RNA-specific structural features\n",
    "    - Adaptive clustering\n",
    "    \"\"\"\n",
    "    similar_seqs = []\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    query_features = _extract_enhanced_rna_features(query_seq)\n",
    "    \n",
    "    # Step 1: Enhanced candidate selection with multi-tier filtering\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        target_id = row['target_id']\n",
    "        train_seq = row['sequence']\n",
    "        \n",
    "        # Skip if coordinates not available\n",
    "        if target_id not in train_coords_dict:\n",
    "            continue\n",
    "        \n",
    "        # Multi-tier length filtering (more permissive for very short/long sequences)\n",
    "        len_ratio = abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq))\n",
    "        if len(query_seq) < 50 or len(train_seq) < 50:  # Short sequences - more permissive\n",
    "            if len_ratio > 0.6:\n",
    "                continue\n",
    "        elif len(query_seq) > 1000 or len(train_seq) > 1000:  # Long sequences - stricter\n",
    "            if len_ratio > 0.2:\n",
    "                continue\n",
    "        else:  # Medium sequences - original threshold\n",
    "            if len_ratio > 0.4:\n",
    "                continue\n",
    "        \n",
    "        # Calculate composite similarity score\n",
    "        composite_score = _calculate_composite_similarity(query_seq, train_seq, query_features)\n",
    "        \n",
    "        if composite_score > 0:  # Only keep sequences with positive similarity\n",
    "            similar_seqs.append((target_id, train_seq, composite_score, train_coords_dict[target_id]))\n",
    "    \n",
    "    # Sort by composite score and take top candidates\n",
    "    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Adaptive candidate selection based on score distribution\n",
    "    candidate_count = min(50, len(similar_seqs))  # Increased initial pool\n",
    "    if len(similar_seqs) > 10:\n",
    "        # Filter out sequences with very low scores (bottom 20%)\n",
    "        score_threshold = np.percentile([x[2] for x in similar_seqs], 80)\n",
    "        filtered_candidates = [x for x in similar_seqs if x[2] >= score_threshold]\n",
    "        candidate_count = min(candidate_count, len(filtered_candidates))\n",
    "        top_candidates = filtered_candidates[:candidate_count]\n",
    "    else:\n",
    "        top_candidates = similar_seqs[:candidate_count]\n",
    "    \n",
    "    # If we have fewer sequences than requested clusters, return all\n",
    "    if len(top_candidates) <= top_n:\n",
    "        return top_candidates[:top_n]\n",
    "    \n",
    "    # Step 2: Enhanced feature matrix for better clustering\n",
    "    feature_matrix = []\n",
    "    for _, seq, _, _ in top_candidates:\n",
    "        features = _extract_enhanced_rna_features(seq)\n",
    "        feature_matrix.append(features)\n",
    "    \n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    \n",
    "    # Step 3: Adaptive clustering\n",
    "    n_clusters = min(top_n, len(top_candidates))\n",
    "    \n",
    "    # Use different clustering approach based on dataset size\n",
    "    if len(top_candidates) >= 15:\n",
    "        # K-means for larger datasets\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "    else:\n",
    "        # Simple diversity-based selection for smaller datasets\n",
    "        cluster_labels = _diversity_based_clustering(feature_matrix, n_clusters)\n",
    "    \n",
    "    # Step 4: Select best representative from each cluster\n",
    "    final_results = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_sequences = [top_candidates[i] for i in range(len(top_candidates)) \n",
    "                           if cluster_labels[i] == cluster_id]\n",
    "        \n",
    "        if cluster_sequences:\n",
    "            # Sort by composite score and take the best one\n",
    "            cluster_sequences.sort(key=lambda x: x[2], reverse=True)\n",
    "            final_results.append(cluster_sequences[0])\n",
    "    \n",
    "    # Sort final results by similarity score\n",
    "    final_results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return final_results[:top_n]\n",
    "\n",
    "def _calculate_composite_similarity(query_seq, train_seq, query_features):\n",
    "    \"\"\"\n",
    "    Calculate composite similarity using multiple alignment methods and features.\n",
    "    \"\"\"\n",
    "    query_seq_obj = Seq(query_seq)\n",
    "    \n",
    "    # 1. Global alignment (original method)\n",
    "    global_alignments = pairwise2.align.globalms(query_seq_obj, train_seq, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "    global_score = 0\n",
    "    if global_alignments:\n",
    "        alignment = global_alignments[0]\n",
    "        global_score = alignment.score / (2 * min(len(query_seq), len(train_seq)))\n",
    "    \n",
    "    # 2. Local alignment for finding similar regions\n",
    "    local_alignments = pairwise2.align.localms(query_seq_obj, train_seq, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "    local_score = 0\n",
    "    if local_alignments:\n",
    "        alignment = local_alignments[0]\n",
    "        local_score = alignment.score / (2 * min(len(query_seq), len(train_seq)))\n",
    "    \n",
    "    # 3. Feature-based similarity\n",
    "    train_features = _extract_enhanced_rna_features(train_seq)\n",
    "    feature_similarity = cosine_similarity([query_features], [train_features])[0][0]\n",
    "    \n",
    "    # 4. K-mer similarity for sequence motifs\n",
    "    kmer_similarity = _calculate_kmer_similarity(query_seq, train_seq, k=3)\n",
    "    \n",
    "    # Weighted composite score\n",
    "    composite_score = (\n",
    "        0.4 * global_score + \n",
    "        0.3 * local_score + \n",
    "        0.2 * feature_similarity + \n",
    "        0.1 * kmer_similarity\n",
    "    )\n",
    "    \n",
    "    return composite_score\n",
    "\n",
    "def _calculate_kmer_similarity(seq1, seq2, k=3):\n",
    "    \"\"\"Calculate k-mer based similarity between sequences.\"\"\"\n",
    "    def get_kmers(seq, k):\n",
    "        return set(seq[i:i+k] for i in range(len(seq) - k + 1))\n",
    "    \n",
    "    kmers1 = get_kmers(seq1.upper(), k)\n",
    "    kmers2 = get_kmers(seq2.upper(), k)\n",
    "    \n",
    "    if not kmers1 or not kmers2:\n",
    "        return 0\n",
    "    \n",
    "    intersection = len(kmers1.intersection(kmers2))\n",
    "    union = len(kmers1.union(kmers2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def _diversity_based_clustering(feature_matrix, n_clusters):\n",
    "    \"\"\"Simple diversity-based clustering for small datasets.\"\"\"\n",
    "    n_samples = len(feature_matrix)\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    if n_samples <= n_clusters:\n",
    "        return np.arange(n_samples)\n",
    "    \n",
    "    # Select diverse representatives\n",
    "    selected_indices = [0]  # Start with first sequence\n",
    "    \n",
    "    for cluster_id in range(1, n_clusters):\n",
    "        max_min_distance = -1\n",
    "        best_idx = -1\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            \n",
    "            # Find minimum distance to already selected sequences\n",
    "            min_distance = min(\n",
    "                np.linalg.norm(feature_matrix[i] - feature_matrix[j]) \n",
    "                for j in selected_indices\n",
    "            )\n",
    "            \n",
    "            if min_distance > max_min_distance:\n",
    "                max_min_distance = min_distance\n",
    "                best_idx = i\n",
    "        \n",
    "        if best_idx != -1:\n",
    "            selected_indices.append(best_idx)\n",
    "    \n",
    "    # Assign remaining sequences to closest cluster centers\n",
    "    for i in range(n_samples):\n",
    "        if i not in selected_indices:\n",
    "            distances = [\n",
    "                np.linalg.norm(feature_matrix[i] - feature_matrix[j]) \n",
    "                for j in selected_indices\n",
    "            ]\n",
    "            cluster_labels[i] = np.argmin(distances)\n",
    "        else:\n",
    "            cluster_labels[i] = selected_indices.index(i)\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "def _extract_enhanced_rna_features(sequence):\n",
    "    \"\"\"\n",
    "    Extract comprehensive RNA-specific features for better clustering and similarity.\n",
    "    \"\"\"\n",
    "    seq = sequence.upper()\n",
    "    features = []\n",
    "    \n",
    "    # 1. Basic nucleotide frequencies\n",
    "    nucleotides = ['A', 'U', 'G', 'C']\n",
    "    for nuc in nucleotides:\n",
    "        freq = seq.count(nuc) / len(seq) if len(seq) > 0 else 0\n",
    "        features.append(freq)\n",
    "    \n",
    "    # 2. Dinucleotide frequencies (reduced set - most important for RNA)\n",
    "    important_dinucs = ['AU', 'UA', 'GC', 'CG', 'GU', 'UG', 'AA', 'UU', 'GG', 'CC']\n",
    "    for dinuc in important_dinucs:\n",
    "        count = 0\n",
    "        for i in range(len(seq) - 1):\n",
    "            if seq[i:i+2] == dinuc:\n",
    "                count += 1\n",
    "        freq = count / (len(seq) - 1) if len(seq) > 1 else 0\n",
    "        features.append(freq)\n",
    "    \n",
    "    # 3. RNA secondary structure indicators\n",
    "    gc_content = (seq.count('G') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    au_content = (seq.count('A') + seq.count('U')) / len(seq) if len(seq) > 0 else 0\n",
    "    purine_content = (seq.count('A') + seq.count('G')) / len(seq) if len(seq) > 0 else 0\n",
    "    pyrimidine_content = (seq.count('U') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    \n",
    "    features.extend([gc_content, au_content, purine_content, pyrimidine_content])\n",
    "    \n",
    "    # 4. Sequence complexity measures\n",
    "    length_normalized = min(len(seq) / 1000.0, 1.0)  # Capped normalization\n",
    "    \n",
    "    # Simple entropy calculation\n",
    "    entropy = 0\n",
    "    for nuc in nucleotides:\n",
    "        freq = seq.count(nuc) / len(seq) if len(seq) > 0 else 0\n",
    "        if freq > 0:\n",
    "            entropy -= freq * np.log2(freq)\n",
    "    entropy_normalized = entropy / 2.0  # Max entropy for 4 nucleotides is 2\n",
    "    \n",
    "    features.extend([length_normalized, entropy_normalized])\n",
    "    \n",
    "    # 5. Repetitive pattern detection\n",
    "    repeat_content = _calculate_repeat_content(seq)\n",
    "    features.append(repeat_content)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def _calculate_repeat_content(sequence):\n",
    "    \"\"\"Calculate the proportion of repetitive content in the sequence.\"\"\"\n",
    "    if len(sequence) < 6:\n",
    "        return 0\n",
    "    \n",
    "    repeat_count = 0\n",
    "    window_size = 3\n",
    "    \n",
    "    for i in range(len(sequence) - window_size + 1):\n",
    "        motif = sequence[i:i + window_size]\n",
    "        # Look for the same motif in the rest of the sequence\n",
    "        for j in range(i + window_size, len(sequence) - window_size + 1):\n",
    "            if sequence[j:j + window_size] == motif:\n",
    "                repeat_count += 1\n",
    "                break\n",
    "    \n",
    "    return repeat_count / (len(sequence) - window_size + 1) if len(sequence) > window_size else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fbfc66",
   "metadata": {
    "_cell_guid": "aaf3f747-0607-4470-a425-dbe714562373",
    "_uuid": "0d532205-a881-4262-86ec-36588b58b8f8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:50.003031Z",
     "iopub.status.busy": "2025-11-29T19:23:50.002634Z",
     "iopub.status.idle": "2025-11-29T19:23:50.016781Z",
     "shell.execute_reply": "2025-11-29T19:23:50.015170Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.178927,
     "end_time": "2025-11-29T19:23:50.018992",
     "exception": false,
     "start_time": "2025-11-29T19:23:49.840065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adaptive_rna_constraints(coordinates, sequence, confidence=1.0):\n",
    "    # Make a copy of coordinates to refine\n",
    "    refined_coords = coordinates.copy()\n",
    "    n_residues = len(sequence)\n",
    "    \n",
    "    # Calculate constraint strength (inverse of confidence)\n",
    "    # High confidence templates receive gentler constraints\n",
    "    constraint_strength = 0.8 * (1.0 - min(confidence, 0.8))\n",
    "    \n",
    "    # 1. Sequential distance constraints (consecutive nucleotides)\n",
    "    # More flexible distance range (statistical distribution from PDB)\n",
    "    seq_min_dist = 5.5  # Minimum sequential distance\n",
    "    seq_max_dist = 6.5  # Maximum sequential distance\n",
    "    \n",
    "    for i in range(n_residues - 1):\n",
    "        current_pos = refined_coords[i]\n",
    "        next_pos = refined_coords[i+1]\n",
    "        \n",
    "        # Calculate current distance\n",
    "        current_dist = np.linalg.norm(next_pos - current_pos)\n",
    "        \n",
    "        # Only adjust if significantly outside expected range\n",
    "        if current_dist < seq_min_dist or current_dist > seq_max_dist:\n",
    "            # Calculate target distance (midpoint of range)\n",
    "            target_dist = (seq_min_dist + seq_max_dist) / 2\n",
    "            \n",
    "            # Get direction vector\n",
    "            direction = next_pos - current_pos\n",
    "            direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "            \n",
    "            # Apply partial adjustment based on constraint strength\n",
    "            adjustment = (target_dist - current_dist) * constraint_strength\n",
    "            \n",
    "            # Only adjust the next position to preserve the overall fold\n",
    "            refined_coords[i+1] = current_pos + direction * (current_dist + adjustment)\n",
    "    \n",
    "    # 2. Steric clash prevention (more conservative)\n",
    "    min_allowed_distance = 3.8  # Minimum distance between non-consecutive C1' atoms\n",
    "    \n",
    "    # Calculate all pairwise distances\n",
    "    dist_matrix = distance_matrix(refined_coords, refined_coords)\n",
    "    \n",
    "    # Find severe clashes (atoms too close)\n",
    "    severe_clashes = np.where((dist_matrix < min_allowed_distance) & (dist_matrix > 0))\n",
    "    \n",
    "    # Fix severe clashes\n",
    "    for idx in range(len(severe_clashes[0])):\n",
    "        i, j = severe_clashes[0][idx], severe_clashes[1][idx]\n",
    "        \n",
    "        # Skip consecutive nucleotides and previously processed pairs\n",
    "        if abs(i - j) <= 1 or i >= j:\n",
    "            continue\n",
    "            \n",
    "        # Get current positions and distance\n",
    "        pos_i = refined_coords[i]\n",
    "        pos_j = refined_coords[j]\n",
    "        current_dist = dist_matrix[i, j]\n",
    "        \n",
    "        # Calculate necessary adjustment but scale by constraint strength\n",
    "        direction = pos_j - pos_i\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "        \n",
    "        # Calculate partial adjustment\n",
    "        adjustment = (min_allowed_distance - current_dist) * constraint_strength\n",
    "        \n",
    "        # Move points apart\n",
    "        refined_coords[i] = pos_i - direction * (adjustment / 2)\n",
    "        refined_coords[j] = pos_j + direction * (adjustment / 2)\n",
    "    \n",
    "    # 3. Very light base-pair constraining (if confidence is low)\n",
    "    if constraint_strength > 0.3:  # Only apply if template confidence is low\n",
    "        # Simple Watson-Crick base pairs\n",
    "        pairs = {'A': 'U', 'U': 'A', 'G': 'C', 'C': 'G'}\n",
    "        \n",
    "        # Scan for potential base pairs\n",
    "        for i in range(n_residues):\n",
    "            base_i = sequence[i]\n",
    "            complement = pairs.get(base_i)\n",
    "            \n",
    "            if not complement:\n",
    "                continue\n",
    "                \n",
    "            # Look for complementary bases within a reasonable range\n",
    "            for j in range(i + 3, min(i + 20, n_residues)):\n",
    "                if sequence[j] == complement:\n",
    "                    # Calculate current distance\n",
    "                    current_dist = np.linalg.norm(refined_coords[i] - refined_coords[j])\n",
    "                    \n",
    "                    # Only consider if distance suggests potential pairing\n",
    "                    if 8.0 < current_dist < 14.0:\n",
    "                        # Target 10.5Å as generic base-pair C1'-C1' distance\n",
    "                        target_dist = 10.5\n",
    "                        \n",
    "                        # Calculate very gentle adjustment (scaled by constraint_strength)\n",
    "                        adjustment = (target_dist - current_dist) * (constraint_strength * 0.3)\n",
    "                        \n",
    "                        # Get direction vector\n",
    "                        direction = refined_coords[j] - refined_coords[i]\n",
    "                        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                        \n",
    "                        # Apply very gentle adjustment to both positions\n",
    "                        refined_coords[i] = refined_coords[i] - direction * (adjustment / 2)\n",
    "                        refined_coords[j] = refined_coords[j] + direction * (adjustment / 2)\n",
    "                        \n",
    "                        # Only consider one potential pair per base (closest match)\n",
    "                        break\n",
    "    \n",
    "    return refined_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea4b2ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:50.395684Z",
     "iopub.status.busy": "2025-11-29T19:23:50.395301Z",
     "iopub.status.idle": "2025-11-29T19:23:50.413130Z",
     "shell.execute_reply": "2025-11-29T19:23:50.411754Z"
    },
    "papermill": {
     "duration": 0.175336,
     "end_time": "2025-11-29T19:23:50.415230",
     "exception": false,
     "start_time": "2025-11-29T19:23:50.239894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adapt_template_to_query(query_seq, template_seq, template_coords, alignment=None):\n",
    "    if alignment is None:\n",
    "        from Bio.Seq import Seq\n",
    "        from Bio import pairwise2\n",
    "        \n",
    "        query_seq_obj = Seq(query_seq)\n",
    "        template_seq_obj = Seq(template_seq)\n",
    "        alignments = pairwise2.align.globalms(query_seq_obj, template_seq_obj, 2.9, -1, -10, -0.5, one_alignment_only=True)\n",
    "        \n",
    "        if not alignments:\n",
    "            return generate_improved_rna_structure(query_seq)\n",
    "            \n",
    "        alignment = alignments[0]\n",
    "    \n",
    "    aligned_query = alignment.seqA\n",
    "    aligned_template = alignment.seqB\n",
    "    \n",
    "    query_coords = np.zeros((len(query_seq), 3))\n",
    "    query_coords.fill(np.nan)\n",
    "    \n",
    "    # Map template coordinates to query\n",
    "    query_idx = 0\n",
    "    template_idx = 0\n",
    "    \n",
    "    for i in range(len(aligned_query)):\n",
    "        query_char = aligned_query[i]\n",
    "        template_char = aligned_template[i]\n",
    "        \n",
    "        if query_char != '-' and template_char != '-':\n",
    "            if template_idx < len(template_coords):\n",
    "                query_coords[query_idx] = template_coords[template_idx]\n",
    "            template_idx += 1\n",
    "            query_idx += 1\n",
    "        elif query_char != '-' and template_char == '-':\n",
    "            query_idx += 1\n",
    "        elif query_char == '-' and template_char != '-':\n",
    "            template_idx += 1\n",
    "    \n",
    "    # IMPROVED GAP FILLING - maintains RNA backbone geometry\n",
    "    backbone_distance = 5.9  # Typical C1'-C1' distance\n",
    "    \n",
    "    # Fill gaps by maintaining realistic backbone connectivity\n",
    "    for i in range(len(query_coords)):\n",
    "        if np.isnan(query_coords[i, 0]):\n",
    "            # Find nearest valid neighbors\n",
    "            prev_valid = next_valid = None\n",
    "            \n",
    "            for j in range(i-1, -1, -1):\n",
    "                if not np.isnan(query_coords[j, 0]):\n",
    "                    prev_valid = j\n",
    "                    break\n",
    "                    \n",
    "            for j in range(i+1, len(query_coords)):\n",
    "                if not np.isnan(query_coords[j, 0]):\n",
    "                    next_valid = j\n",
    "                    break\n",
    "            \n",
    "            if prev_valid is not None and next_valid is not None:\n",
    "                # Interpolate along realistic RNA backbone path\n",
    "                gap_size = next_valid - prev_valid\n",
    "                total_distance = np.linalg.norm(query_coords[next_valid] - query_coords[prev_valid])\n",
    "                expected_distance = gap_size * backbone_distance\n",
    "                \n",
    "                # If gap is compressed, extend it realistically\n",
    "                if total_distance < expected_distance * 0.7:\n",
    "                    direction = query_coords[next_valid] - query_coords[prev_valid]\n",
    "                    direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                    \n",
    "                    # Place intermediate points along extended path\n",
    "                    for k, idx in enumerate(range(prev_valid + 1, next_valid)):\n",
    "                        progress = (k + 1) / gap_size\n",
    "                        base_pos = query_coords[prev_valid] + direction * expected_distance * progress\n",
    "                        \n",
    "                        # Add slight curvature for realism\n",
    "                        perpendicular = np.cross(direction, [0, 0, 1])\n",
    "                        if np.linalg.norm(perpendicular) < 1e-6:\n",
    "                            perpendicular = np.cross(direction, [1, 0, 0])\n",
    "                        perpendicular = perpendicular / (np.linalg.norm(perpendicular) + 1e-10)\n",
    "                        \n",
    "                        curve_amplitude = 2.0 * np.sin(progress * np.pi)\n",
    "                        query_coords[idx] = base_pos + perpendicular * curve_amplitude\n",
    "                else:\n",
    "                    # Linear interpolation for normal gaps\n",
    "                    for k, idx in enumerate(range(prev_valid + 1, next_valid)):\n",
    "                        weight = (k + 1) / gap_size\n",
    "                        query_coords[idx] = (1 - weight) * query_coords[prev_valid] + weight * query_coords[next_valid]\n",
    "            \n",
    "            elif prev_valid is not None:\n",
    "                # Extend from previous position\n",
    "                if prev_valid > 0 and not np.isnan(query_coords[prev_valid-1, 0]):\n",
    "                    direction = query_coords[prev_valid] - query_coords[prev_valid-1]\n",
    "                    direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                else:\n",
    "                    direction = np.array([1.0, 0.0, 0.0])\n",
    "                \n",
    "                steps_needed = i - prev_valid\n",
    "                for step in range(1, steps_needed + 1):\n",
    "                    pos_idx = prev_valid + step\n",
    "                    if pos_idx < len(query_coords):\n",
    "                        query_coords[pos_idx] = query_coords[prev_valid] + direction * backbone_distance * step\n",
    "            \n",
    "            elif next_valid is not None:\n",
    "                # Work backwards from next position\n",
    "                direction = np.array([-1.0, 0.0, 0.0])  # Default backward direction\n",
    "                steps_needed = next_valid - i\n",
    "                for step in range(steps_needed, 0, -1):\n",
    "                    pos_idx = next_valid - step\n",
    "                    if pos_idx >= 0:\n",
    "                        query_coords[pos_idx] = query_coords[next_valid] - direction * backbone_distance * step\n",
    "    \n",
    "    # Final cleanup\n",
    "    query_coords = np.nan_to_num(query_coords)\n",
    "    return query_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ad2684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:50.728076Z",
     "iopub.status.busy": "2025-11-29T19:23:50.727676Z",
     "iopub.status.idle": "2025-11-29T19:23:50.741180Z",
     "shell.execute_reply": "2025-11-29T19:23:50.740011Z"
    },
    "papermill": {
     "duration": 0.172457,
     "end_time": "2025-11-29T19:23:50.743178",
     "exception": false,
     "start_time": "2025-11-29T19:23:50.570721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_improved_rna_structure(sequence):\n",
    "    \"\"\"\n",
    "    Generate a more realistic RNA structure fallback based on sequence patterns\n",
    "    and basic RNA structure principles.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        \n",
    "    Returns:\n",
    "        Array of 3D coordinates\n",
    "    \"\"\"\n",
    "    n_residues = len(sequence)\n",
    "    coordinates = np.zeros((n_residues, 3))\n",
    "    \n",
    "    # Analyze sequence to predict structural elements\n",
    "    # Look for complementary regions that could form base pairs\n",
    "    potential_stems = identify_potential_stems(sequence)\n",
    "    \n",
    "    # Default parameters\n",
    "    radius_helix = 10.0\n",
    "    radius_loop = 15.0\n",
    "    rise_per_residue_helix = 2.5\n",
    "    rise_per_residue_loop = 1.5\n",
    "    angle_per_residue_helix = 0.6\n",
    "    angle_per_residue_loop = 0.3\n",
    "    \n",
    "    # Assign structural classifications\n",
    "    structure_types = assign_structure_types(sequence, potential_stems)\n",
    "    \n",
    "    # Generate coordinates based on predicted structure\n",
    "    current_pos = np.array([0.0, 0.0, 0.0])\n",
    "    current_direction = np.array([0.0, 0.0, 1.0])\n",
    "    current_angle = 0.0\n",
    "    \n",
    "    for i in range(n_residues):\n",
    "        if structure_types[i] == 'stem':\n",
    "            # Part of a helical stem\n",
    "            current_angle += angle_per_residue_helix\n",
    "            coordinates[i] = [\n",
    "                radius_helix * np.cos(current_angle), \n",
    "                radius_helix * np.sin(current_angle), \n",
    "                current_pos[2] + rise_per_residue_helix\n",
    "            ]\n",
    "            current_pos = coordinates[i]\n",
    "        elif structure_types[i] == 'loop':\n",
    "            # Part of a loop\n",
    "            current_angle += angle_per_residue_loop\n",
    "            z_shift = rise_per_residue_loop * np.sin(current_angle * 0.5)\n",
    "            coordinates[i] = [\n",
    "                radius_loop * np.cos(current_angle), \n",
    "                radius_loop * np.sin(current_angle), \n",
    "                current_pos[2] + z_shift\n",
    "            ]\n",
    "            current_pos = coordinates[i]\n",
    "        else:\n",
    "            # Single-stranded region\n",
    "            # Add some randomness to make it look more realistic\n",
    "            jitter = np.random.normal(0, 1, 3) * 2.0\n",
    "            coordinates[i] = current_pos + jitter\n",
    "            current_pos = coordinates[i]\n",
    "            \n",
    "    return coordinates\n",
    "\n",
    "def identify_potential_stems(sequence):\n",
    "    \"\"\"\n",
    "    Identify potential stem regions by looking for self-complementary segments.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (start1, end1, start2, end2) representing potentially paired regions\n",
    "    \"\"\"\n",
    "    complementary_bases = {'A': 'U', 'U': 'A', 'G': 'C', 'C': 'G'}\n",
    "    min_stem_length = 3\n",
    "    potential_stems = []\n",
    "    \n",
    "    # Simple stem identification\n",
    "    for i in range(len(sequence) - min_stem_length):\n",
    "        for j in range(i + min_stem_length + 3, len(sequence) - min_stem_length + 1):\n",
    "            # Check if regions could form a stem\n",
    "            potential_stem_len = min(min_stem_length, len(sequence) - j)\n",
    "            is_stem = True\n",
    "            \n",
    "            for k in range(potential_stem_len):\n",
    "                if sequence[i+k] not in complementary_bases or \\\n",
    "                   complementary_bases[sequence[i+k]] != sequence[j+potential_stem_len-k-1]:\n",
    "                    is_stem = False\n",
    "                    break\n",
    "            \n",
    "            if is_stem:\n",
    "                potential_stems.append((i, i+potential_stem_len-1, j, j+potential_stem_len-1))\n",
    "    \n",
    "    return potential_stems\n",
    "\n",
    "def assign_structure_types(sequence, potential_stems):\n",
    "    \"\"\"\n",
    "    Assign each nucleotide to a structural element type.\n",
    "    \n",
    "    Args:\n",
    "        sequence: RNA sequence string\n",
    "        potential_stems: List of tuples representing stem regions\n",
    "        \n",
    "    Returns:\n",
    "        List of structure types ('stem', 'loop', 'single')\n",
    "    \"\"\"\n",
    "    structure_types = ['single'] * len(sequence)\n",
    "    \n",
    "    # Mark stem regions\n",
    "    for stem in potential_stems:\n",
    "        start1, end1, start2, end2 = stem\n",
    "        for i in range(end1 - start1 + 1):\n",
    "            structure_types[start1 + i] = 'stem'\n",
    "            structure_types[end2 - i] = 'stem'\n",
    "    \n",
    "    # Mark loop regions (regions between paired regions)\n",
    "    for i in range(len(potential_stems) - 1):\n",
    "        _, end1, start2, _ = potential_stems[i]\n",
    "        next_start1, _, _, _ = potential_stems[i+1]\n",
    "        \n",
    "        if next_start1 > end1 + 1 and start2 > next_start1:\n",
    "            for j in range(end1 + 1, next_start1):\n",
    "                structure_types[j] = 'loop'\n",
    "    \n",
    "    return structure_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e9b62a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:51.052759Z",
     "iopub.status.busy": "2025-11-29T19:23:51.052419Z",
     "iopub.status.idle": "2025-11-29T19:23:51.064523Z",
     "shell.execute_reply": "2025-11-29T19:23:51.063358Z"
    },
    "papermill": {
     "duration": 0.169655,
     "end_time": "2025-11-29T19:23:51.066672",
     "exception": false,
     "start_time": "2025-11-29T19:23:50.897017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a more realistic RNA structure when no good templates are found\n",
    "def generate_rna_structure(sequence, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    n_residues = len(sequence)\n",
    "    coordinates = np.zeros((n_residues, 3))\n",
    "    \n",
    "    # Initialize the first few residues in a helix\n",
    "    for i in range(min(3, n_residues)):\n",
    "        angle = i * 0.6\n",
    "        coordinates[i] = [10.0 * np.cos(angle), 10.0 * np.sin(angle), i * 2.5]\n",
    "    \n",
    "    # Add more complex folding patterns\n",
    "    current_direction = np.array([0.0, 0.0, 1.0])  # Start moving along z-axis\n",
    "    \n",
    "    # Define base-pairing tendencies (G-C and A-U pairs)\n",
    "    for i in range(3, n_residues):\n",
    "        # Check for potential base-pairing in the sequence\n",
    "        has_pair = False\n",
    "        pair_idx = -1\n",
    "        \n",
    "        # Simple detection of complementary bases (G-C, A-U)\n",
    "        complementary = {'G': 'C', 'C': 'G', 'A': 'U', 'U': 'A'}\n",
    "        current_base = sequence[i]\n",
    "        \n",
    "        # Look for potential base-pairing within a window before the current position\n",
    "        window_size = min(i, 15)  # Look back up to 15 bases\n",
    "        for j in range(i-window_size, i):\n",
    "            if j >= 0 and sequence[j] == complementary.get(current_base, 'X'):\n",
    "                # Found a potential pair\n",
    "                has_pair = True\n",
    "                pair_idx = j\n",
    "                break\n",
    "        \n",
    "        if has_pair and i - pair_idx <= 10 and random.random() < 0.7:\n",
    "            # Try to create a base-pair by positioning this nucleotide near its pair\n",
    "            pair_pos = coordinates[pair_idx]\n",
    "            \n",
    "            # Create a position that's roughly opposite to the pair\n",
    "            random_offset = np.random.normal(0, 1, 3) * 2.0\n",
    "            base_pair_distance = 10.0 + random.uniform(-1.0, 1.0)\n",
    "            \n",
    "            # Calculate a vector from base-pair toward center of structure\n",
    "            center = np.mean(coordinates[:i], axis=0)\n",
    "            direction = center - pair_pos\n",
    "            direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "            \n",
    "            # Position new nucleotide in the general direction of the \"center\"\n",
    "            coordinates[i] = pair_pos + direction * base_pair_distance + random_offset\n",
    "            \n",
    "            # Update direction for next nucleotide\n",
    "            current_direction = np.random.normal(0, 0.3, 3)\n",
    "            current_direction = current_direction / (np.linalg.norm(current_direction) + 1e-10)\n",
    "            \n",
    "        else:\n",
    "            # No base-pairing detected, continue with the current fold direction\n",
    "            # Randomly rotate current direction to simulate RNA flexibility\n",
    "            if random.random() < 0.3:\n",
    "                # More significant direction change\n",
    "                angle = random.uniform(0.2, 0.6)\n",
    "                axis = np.random.normal(0, 1, 3)\n",
    "                axis = axis / (np.linalg.norm(axis) + 1e-10)\n",
    "                rotation = R.from_rotvec(angle * axis)\n",
    "                current_direction = rotation.apply(current_direction)\n",
    "            else:\n",
    "                # Small random changes in direction\n",
    "                current_direction += np.random.normal(0, 0.15, 3)\n",
    "                current_direction = current_direction / (np.linalg.norm(current_direction) + 1e-10)\n",
    "            \n",
    "            # Distance between consecutive nucleotides (3.5-4.5Å is typical)\n",
    "            step_size = random.uniform(3.5, 4.5)\n",
    "            \n",
    "            # Update position\n",
    "            coordinates[i] = coordinates[i-1] + step_size * current_direction\n",
    "    \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94699e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:51.379389Z",
     "iopub.status.busy": "2025-11-29T19:23:51.378954Z",
     "iopub.status.idle": "2025-11-29T19:23:51.386554Z",
     "shell.execute_reply": "2025-11-29T19:23:51.385420Z"
    },
    "papermill": {
     "duration": 0.16795,
     "end_time": "2025-11-29T19:23:51.388160",
     "exception": false,
     "start_time": "2025-11-29T19:23:51.220210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rna_structures(sequence, target_id, train_seqs_df, train_coords_dict, n_predictions=5):\n",
    "    predictions = []\n",
    "    \n",
    "    # Find similar sequences in the training data\n",
    "    similar_seqs = find_similar_sequences(sequence, train_seqs_df, train_coords_dict, top_n=n_predictions)\n",
    "    \n",
    "    # If we found any similar sequences, use them as templates\n",
    "    if similar_seqs:\n",
    "        for i, (template_id, template_seq, similarity_score, template_coords) in enumerate(similar_seqs):\n",
    "            # Adapt template coordinates to the query sequence\n",
    "            adapted_coords = adapt_template_to_query(sequence, template_seq, template_coords)\n",
    "            \n",
    "            if adapted_coords is not None:\n",
    "                # Apply adaptive constraints based on template similarity\n",
    "                # For high similarity templates, apply very gentle constraints\n",
    "                refined_coords = adaptive_rna_constraints(adapted_coords, sequence, confidence=similarity_score)\n",
    "                \n",
    "                # Add some randomness (less for better templates)\n",
    "                random_scale = max(0.05, 0.8 - similarity_score)  # Reduced randomness\n",
    "                randomized_coords = refined_coords.copy()\n",
    "                randomized_coords += np.random.normal(0, random_scale, randomized_coords.shape)\n",
    "                \n",
    "                predictions.append(randomized_coords)\n",
    "                \n",
    "                if len(predictions) >= n_predictions:\n",
    "                    break\n",
    "    \n",
    "    # If we don't have enough predictions from templates, generate de novo structures\n",
    "    while len(predictions) < n_predictions:\n",
    "        seed_value = hash(target_id) % 10000 + len(predictions) * 1000\n",
    "        de_novo_coords = generate_rna_structure(sequence, seed=seed_value)\n",
    "        \n",
    "        # Apply stronger constraints to de novo structures (lower confidence)\n",
    "        refined_de_novo = adaptive_rna_constraints(de_novo_coords, sequence, confidence=0.2)\n",
    "        \n",
    "        predictions.append(refined_de_novo)\n",
    "    \n",
    "    return predictions[:n_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c913e7fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:23:51.697490Z",
     "iopub.status.busy": "2025-11-29T19:23:51.697067Z",
     "iopub.status.idle": "2025-11-29T19:30:59.128563Z",
     "shell.execute_reply": "2025-11-29T19:30:59.127364Z"
    },
    "papermill": {
     "duration": 427.75598,
     "end_time": "2025-11-29T19:30:59.295692",
     "exception": false,
     "start_time": "2025-11-29T19:23:51.539712",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 1/12: R1107 (69 nt), elapsed: 0.0s, est. remaining: 0.0s\n",
      "Processing target 6/12: R1128 (238 nt), elapsed: 125.8s, est. remaining: 125.8s\n",
      "Processing target 11/12: R1189 (118 nt), elapsed: 335.5s, est. remaining: 30.5s\n",
      "Generated predictions for 12 RNA sequences\n",
      "Total runtime: 427.4 seconds\n"
     ]
    }
   ],
   "source": [
    "# List to store all prediction records\n",
    "all_predictions = []\n",
    "\n",
    "# Set up time tracking\n",
    "start_time = time.time()\n",
    "total_targets = len(test_seqs)\n",
    "\n",
    "# For each sequence in the test set\n",
    "for idx, row in test_seqs.iterrows():\n",
    "    target_id = row['target_id']\n",
    "    sequence = row['sequence']\n",
    "    \n",
    "    # Progress tracking\n",
    "    if idx % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        targets_processed = idx + 1\n",
    "        if targets_processed > 0:\n",
    "            avg_time_per_target = elapsed / targets_processed\n",
    "            est_time_remaining = avg_time_per_target * (total_targets - targets_processed)\n",
    "            print(f\"Processing target {targets_processed}/{total_targets}: {target_id} ({len(sequence)} nt), \"\n",
    "                  f\"elapsed: {elapsed:.1f}s, est. remaining: {est_time_remaining:.1f}s\")\n",
    "    \n",
    "    # Generate 5 different structure predictions\n",
    "    predictions = predict_rna_structures(sequence, target_id, train_seqs_extended, train_coords_dict, n_predictions=5)\n",
    "    \n",
    "    # For each residue in the sequence\n",
    "    for j in range(len(sequence)):\n",
    "        pred_row = {\n",
    "            'ID': f\"{target_id}_{j+1}\",\n",
    "            'resname': sequence[j],\n",
    "            'resid': j + 1\n",
    "        }\n",
    "        \n",
    "        # Add coordinates from all 5 predictions\n",
    "        for i in range(5):\n",
    "            pred_row[f'x_{i+1}'] = predictions[i][j][0]\n",
    "            pred_row[f'y_{i+1}'] = predictions[i][j][1]\n",
    "            pred_row[f'z_{i+1}'] = predictions[i][j][2]\n",
    "        \n",
    "        all_predictions.append(pred_row)\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "submission_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Ensure the submission file has the correct format\n",
    "column_order = ['ID', 'resname', 'resid']\n",
    "for i in range(1, 6):\n",
    "    for coord in ['x', 'y', 'z']:\n",
    "        column_order.append(f'{coord}_{i}')\n",
    "submission_df = submission_df[column_order]\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(f\"Generated predictions for {len(test_seqs)} RNA sequences\")\n",
    "print(f\"Total runtime: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0853474d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:30:59.617830Z",
     "iopub.status.busy": "2025-11-29T19:30:59.617411Z",
     "iopub.status.idle": "2025-11-29T19:30:59.640542Z",
     "shell.execute_reply": "2025-11-29T19:30:59.639422Z"
    },
    "papermill": {
     "duration": 0.186628,
     "end_time": "2025-11-29T19:30:59.642404",
     "exception": false,
     "start_time": "2025-11-29T19:30:59.455776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>z_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>z_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>z_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R1107_1</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.564016</td>\n",
       "      <td>8.506023</td>\n",
       "      <td>8.563398</td>\n",
       "      <td>29.807484</td>\n",
       "      <td>27.692209</td>\n",
       "      <td>8.479686</td>\n",
       "      <td>-6.296189</td>\n",
       "      <td>-39.834560</td>\n",
       "      <td>-37.766381</td>\n",
       "      <td>-8.797112</td>\n",
       "      <td>19.282954</td>\n",
       "      <td>44.151069</td>\n",
       "      <td>17.288640</td>\n",
       "      <td>12.656922</td>\n",
       "      <td>80.880359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1107_2</td>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.812276</td>\n",
       "      <td>10.505832</td>\n",
       "      <td>13.995445</td>\n",
       "      <td>24.127710</td>\n",
       "      <td>26.117311</td>\n",
       "      <td>7.770464</td>\n",
       "      <td>-4.506343</td>\n",
       "      <td>-36.874775</td>\n",
       "      <td>-32.731516</td>\n",
       "      <td>-10.277863</td>\n",
       "      <td>24.188524</td>\n",
       "      <td>40.961668</td>\n",
       "      <td>13.028276</td>\n",
       "      <td>14.734326</td>\n",
       "      <td>77.766398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R1107_3</td>\n",
       "      <td>G</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.900333</td>\n",
       "      <td>14.833828</td>\n",
       "      <td>17.574006</td>\n",
       "      <td>18.305129</td>\n",
       "      <td>24.289402</td>\n",
       "      <td>7.431945</td>\n",
       "      <td>-6.691200</td>\n",
       "      <td>-34.196555</td>\n",
       "      <td>-29.060239</td>\n",
       "      <td>-10.024333</td>\n",
       "      <td>28.011798</td>\n",
       "      <td>36.507357</td>\n",
       "      <td>9.823407</td>\n",
       "      <td>17.340051</td>\n",
       "      <td>74.356315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R1107_4</td>\n",
       "      <td>G</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.794813</td>\n",
       "      <td>20.088207</td>\n",
       "      <td>18.697806</td>\n",
       "      <td>13.159219</td>\n",
       "      <td>22.809961</td>\n",
       "      <td>5.941215</td>\n",
       "      <td>-11.261826</td>\n",
       "      <td>-33.813259</td>\n",
       "      <td>-26.515221</td>\n",
       "      <td>-6.673572</td>\n",
       "      <td>30.053245</td>\n",
       "      <td>32.447846</td>\n",
       "      <td>9.513785</td>\n",
       "      <td>22.820188</td>\n",
       "      <td>72.338473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R1107_5</td>\n",
       "      <td>G</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.803156</td>\n",
       "      <td>25.493693</td>\n",
       "      <td>17.101248</td>\n",
       "      <td>7.666886</td>\n",
       "      <td>20.454284</td>\n",
       "      <td>5.224429</td>\n",
       "      <td>-14.992587</td>\n",
       "      <td>-36.029554</td>\n",
       "      <td>-23.212102</td>\n",
       "      <td>-3.869055</td>\n",
       "      <td>28.692063</td>\n",
       "      <td>29.112633</td>\n",
       "      <td>11.578025</td>\n",
       "      <td>27.957970</td>\n",
       "      <td>71.090324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>R1190_114</td>\n",
       "      <td>U</td>\n",
       "      <td>114</td>\n",
       "      <td>83.938077</td>\n",
       "      <td>106.125683</td>\n",
       "      <td>76.430105</td>\n",
       "      <td>140.500933</td>\n",
       "      <td>172.679404</td>\n",
       "      <td>35.768056</td>\n",
       "      <td>53.180417</td>\n",
       "      <td>5.102236</td>\n",
       "      <td>40.816683</td>\n",
       "      <td>-2.613528</td>\n",
       "      <td>-45.746450</td>\n",
       "      <td>29.567253</td>\n",
       "      <td>12.377488</td>\n",
       "      <td>171.626533</td>\n",
       "      <td>72.788045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>R1190_115</td>\n",
       "      <td>U</td>\n",
       "      <td>115</td>\n",
       "      <td>83.618960</td>\n",
       "      <td>100.848925</td>\n",
       "      <td>76.779855</td>\n",
       "      <td>143.407460</td>\n",
       "      <td>177.456509</td>\n",
       "      <td>37.153628</td>\n",
       "      <td>52.505848</td>\n",
       "      <td>11.367849</td>\n",
       "      <td>40.355076</td>\n",
       "      <td>-3.072921</td>\n",
       "      <td>-49.986408</td>\n",
       "      <td>33.742916</td>\n",
       "      <td>11.299528</td>\n",
       "      <td>168.999237</td>\n",
       "      <td>67.240029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>R1190_116</td>\n",
       "      <td>U</td>\n",
       "      <td>116</td>\n",
       "      <td>83.275367</td>\n",
       "      <td>95.341510</td>\n",
       "      <td>75.273496</td>\n",
       "      <td>146.259845</td>\n",
       "      <td>182.228885</td>\n",
       "      <td>38.340481</td>\n",
       "      <td>50.016880</td>\n",
       "      <td>5.992516</td>\n",
       "      <td>41.455940</td>\n",
       "      <td>-3.615245</td>\n",
       "      <td>-54.298869</td>\n",
       "      <td>38.296058</td>\n",
       "      <td>9.353374</td>\n",
       "      <td>165.698762</td>\n",
       "      <td>64.915877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>R1190_117</td>\n",
       "      <td>U</td>\n",
       "      <td>117</td>\n",
       "      <td>85.518738</td>\n",
       "      <td>91.352711</td>\n",
       "      <td>71.798746</td>\n",
       "      <td>149.440982</td>\n",
       "      <td>187.442965</td>\n",
       "      <td>39.907901</td>\n",
       "      <td>52.490312</td>\n",
       "      <td>10.830265</td>\n",
       "      <td>37.506683</td>\n",
       "      <td>-4.178282</td>\n",
       "      <td>-58.383911</td>\n",
       "      <td>42.165191</td>\n",
       "      <td>3.985485</td>\n",
       "      <td>164.037062</td>\n",
       "      <td>63.402050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>R1190_118</td>\n",
       "      <td>U</td>\n",
       "      <td>118</td>\n",
       "      <td>92.261625</td>\n",
       "      <td>85.878953</td>\n",
       "      <td>72.681099</td>\n",
       "      <td>152.330444</td>\n",
       "      <td>192.176703</td>\n",
       "      <td>41.219635</td>\n",
       "      <td>55.486639</td>\n",
       "      <td>12.537747</td>\n",
       "      <td>33.716496</td>\n",
       "      <td>-5.323944</td>\n",
       "      <td>-62.585295</td>\n",
       "      <td>46.112408</td>\n",
       "      <td>0.425731</td>\n",
       "      <td>162.103561</td>\n",
       "      <td>56.717304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2515 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID resname  resid        x_1         y_1        z_1         x_2  \\\n",
       "0       R1107_1       G      1  -5.564016    8.506023   8.563398   29.807484   \n",
       "1       R1107_2       G      2  -5.812276   10.505832  13.995445   24.127710   \n",
       "2       R1107_3       G      3  -5.900333   14.833828  17.574006   18.305129   \n",
       "3       R1107_4       G      4  -5.794813   20.088207  18.697806   13.159219   \n",
       "4       R1107_5       G      5  -5.803156   25.493693  17.101248    7.666886   \n",
       "...         ...     ...    ...        ...         ...        ...         ...   \n",
       "2510  R1190_114       U    114  83.938077  106.125683  76.430105  140.500933   \n",
       "2511  R1190_115       U    115  83.618960  100.848925  76.779855  143.407460   \n",
       "2512  R1190_116       U    116  83.275367   95.341510  75.273496  146.259845   \n",
       "2513  R1190_117       U    117  85.518738   91.352711  71.798746  149.440982   \n",
       "2514  R1190_118       U    118  92.261625   85.878953  72.681099  152.330444   \n",
       "\n",
       "             y_2        z_2        x_3        y_3        z_3        x_4  \\\n",
       "0      27.692209   8.479686  -6.296189 -39.834560 -37.766381  -8.797112   \n",
       "1      26.117311   7.770464  -4.506343 -36.874775 -32.731516 -10.277863   \n",
       "2      24.289402   7.431945  -6.691200 -34.196555 -29.060239 -10.024333   \n",
       "3      22.809961   5.941215 -11.261826 -33.813259 -26.515221  -6.673572   \n",
       "4      20.454284   5.224429 -14.992587 -36.029554 -23.212102  -3.869055   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "2510  172.679404  35.768056  53.180417   5.102236  40.816683  -2.613528   \n",
       "2511  177.456509  37.153628  52.505848  11.367849  40.355076  -3.072921   \n",
       "2512  182.228885  38.340481  50.016880   5.992516  41.455940  -3.615245   \n",
       "2513  187.442965  39.907901  52.490312  10.830265  37.506683  -4.178282   \n",
       "2514  192.176703  41.219635  55.486639  12.537747  33.716496  -5.323944   \n",
       "\n",
       "            y_4        z_4        x_5         y_5        z_5  \n",
       "0     19.282954  44.151069  17.288640   12.656922  80.880359  \n",
       "1     24.188524  40.961668  13.028276   14.734326  77.766398  \n",
       "2     28.011798  36.507357   9.823407   17.340051  74.356315  \n",
       "3     30.053245  32.447846   9.513785   22.820188  72.338473  \n",
       "4     28.692063  29.112633  11.578025   27.957970  71.090324  \n",
       "...         ...        ...        ...         ...        ...  \n",
       "2510 -45.746450  29.567253  12.377488  171.626533  72.788045  \n",
       "2511 -49.986408  33.742916  11.299528  168.999237  67.240029  \n",
       "2512 -54.298869  38.296058   9.353374  165.698762  64.915877  \n",
       "2513 -58.383911  42.165191   3.985485  164.037062  63.402050  \n",
       "2514 -62.585295  46.112408   0.425731  162.103561  56.717304  \n",
       "\n",
       "[2515 rows x 18 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabf90b",
   "metadata": {
    "papermill": {
     "duration": 0.155019,
     "end_time": "2025-11-29T19:30:59.959680",
     "exception": false,
     "start_time": "2025-11-29T19:30:59.804661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12276181,
     "sourceId": 87793,
     "sourceType": "competition"
    },
    {
     "datasetId": 7306643,
     "sourceId": 11644010,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7526656,
     "sourceId": 11969392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1215.496991,
   "end_time": "2025-11-29T19:31:02.740329",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T19:10:47.243338",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
